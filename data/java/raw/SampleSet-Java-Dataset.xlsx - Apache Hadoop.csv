Evaluator,Reviewer 1,Reviewer 2,Project,Class,Comment,comment length,summary,Expand,rational,deprecation,usage,exception,todo,Incomplete,Commented code,directive,formatter,License,Ownership,Pointer,Auto generated,Noise,Warning,Recommendation,Precondition,Coding Guidelines,Extension,Subclass explnation,Observation
E4,E1 (agree),,hadoop,TonyRuntimeFactory.java,* Implementation of RuntimeFactory with Tony Runtime,1-3,Implementation of RuntimeFactory with Tony Runtime,,,,,,,,,,,,,Implementation of RuntimeFactory with Tony Runtime,,,,,,,,,
E4,E1 (agree),,,TestEditLogFileOutputStream.java,* Test the EditLogFileOutputStream,1-3,Test the EditLogFileOutputStream,,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,TestMetricsVisitor.java,* Test the metric visitor interface,1-3,Test the metric visitor interface,,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,TestS3BucketManager.java,* Tests for S3 Bucket Manager.,1-3,Tests for S3 Bucket Manager.,,,,,,,,,,,,,Tests for S3 Bucket Manager.,,,,,,,,,
E4,E1 (agree),,,TestKeyInputStream.java,* Tests {@link KeyInputStream}.,1-3,Tests {@link KeyInputStream}.,,,,,,,,,,,,,Tests {@link KeyInputStream}.,,,,,,,,,
E4,E1 (agree),,,ReInitializeContainerRequestPBImpl.java,CHECKSTYLE:OFF,1-3,,,,,,,,,,CHECKSTYLE:OFF,,,,,,,,,,,,,
E4,E1 (agree),,,LRUCacheHashMap.java,* LRU cache with a configurable maximum cache size and access order.,1-3,LRU cache with a configurable maximum cache size and access order.,LRU cache with a configurable maximum cache size and access order.,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,TestDataDrivenDBInputFormat.java,"import org.apache.hadoop.examples.DBCountPageView;|
* Test aspects of DataDrivenDBInputFormat",1-3,Test aspects of DataDrivenDBInputFormat,,,,,,,,import org.apache.hadoop.examples.DBCountPageView;|,,,,,,,,,,,,,,
E4,E1 (agree),,,TestMetricsVisitor.java,* Test the metric visitor interface,1-3,Test the metric visitor interface,,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,VolumeSet.java,* VolumeSet to manage HDDS volumes in a DataNode.,1-3,VolumeSet to manage HDDS volumes in a DataNode.,,,,,,,,,,,,,VolumeSet to manage HDDS volumes in a DataNode.,,,,,,,,,
E4,E1 (agree),,,MetaBlockAlreadyExists.java,* Exception - Meta Block with the same name already exists.,1-3,Exception - Meta Block with the same name already exists.,,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,LogWebService.java,* Support only ATSv2 client only.,1-3,,,,Support only ATSv2 client only,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,TestAddBlock.java,* Test AddBlockOp is written and read correctly,1-3,Test AddBlockOp is written and read correctly,Test AddBlockOp is written and read correctly,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,ProcessIdFileReader.java,* Helper functionality to read the pid from a file.,1-3,Helper functionality to read the pid from a file.,,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,Parser.java,A class for parsing outputs,1-3,A class for parsing outputs,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (disagree),,HashResolver.java,* Order the destinations based on consistent hashing.,1-3,Order the destinations based on consistent hashing.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (disagree),,GetNamespaceInfoResponse.java,* API response for listing HDFS namespaces present in the state store.,1-3,API response for listing HDFS namespaces present in the state store.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,TestSCMDatanodeHeartbeatDispatcher.java,* This class tests the behavior of SCMDatanodeHeartbeatDispatcher.,1-3,This class tests the behavior of SCMDatanodeHeartbeatDispatcher.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,TestOmMultipartKeyInfoCodec.java,* This class tests OmMultipartKeyInfoCodec.,1-3,This class tests OmMultipartKeyInfoCodec.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,TestRouterWebServiceUtil.java,* Test class to validate RouterWebServiceUtil methods.,1-3,Test class to validate RouterWebServiceUtil methods.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,TableConfig.java,* Class that maintains Table Configuration.,1-3,Class that maintains Table Configuration.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,DelegationTokenIdentifier.java,* A delegation token identifier that is specific to HDFS.,1-3,A delegation token identifier that is specific to HDFS.,A delegation token identifier that is specific to HDFS.,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,HttpHeaderConfigurations.java,* Responsible to keep all abfs http headers here.,1-3,,Responsible to keep all abfs http headers here.,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,TestOzoneNativeAuthorizer.java,* Test class for {@link OzoneNativeAuthorizer}.,1-3,Test class for {@link OzoneNativeAuthorizer}.,,,,,,,,,,,,,Test class for {@link OzoneNativeAuthorizer}.,,,,,,,,,
E4,,E3 (agree),,TestAdlContractDeleteLive.java,* Test delete contract test.,1-3,Test delete contract test.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,ConfigurationException.java,* Exception to throw in case of a configuration problem.,1-3,Exception to throw in case of a configuration problem.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,AbstractS3ACommitterFactory.java,* Dynamically create the output committer based on subclass type and settings.,1-3,,Dynamically create the output committer based on subclass type and settings.,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,TestRDBStore.java,* RDBStore Tests.,1-3,RDBStore Tests.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,NullOutputFormat.java,* Consume all outputs and put them in /dev/null.,1-3,Consume all outputs and put them in /dev/null.,Consume all outputs and put them in /dev/null.,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,DefaultAnonymizingRumenSerializer.java,* Default Rumen JSON serializer.,1-3,Default Rumen JSON serializer.,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,QueueName.java,* Represents a queue name.,1-3,Represents a queue name.,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,TestAdlContractSeekLive.java,* Test seek operation on Adl file system.,1-3,Test seek operation on Adl file system.,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,YarnServiceJobSubmitter.java,* Submit a job to cluster.,1-3,Submit a job to cluster.,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,TimelineStorageMonitor.java,"* This abstract class is for monitoring Health of Timeline Storage.
| Different Storages supported by ATSV2.",1-3,This abstract class is for monitoring Health of Timeline Storage,,for monitoring Health of Timeline Storage,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,TestFederationPolicyUtils.java,* Unit test for {@link FederationPolicyUtils}.,1-3,Unit test for {@link FederationPolicyUtils}.,,,,,,,,,,,,,Unit test for {@link FederationPolicyUtils}.,,,,,,,,,
E2,E1 (agree),,,LocalizationStatusPBImpl.java,* PB Impl of {@link LocalizationStatus}.,1-3,PB Impl of {@link LocalizationStatus}.,,,,,,,,,,,,,PB Impl of {@link LocalizationStatus}.,,,,,,,,,
E2,E1 (agree),,,StandardSocketFactory.java,* Specialized SocketFactory to create sockets with a SOCKS proxy,1-3,Specialized SocketFactory to create sockets with a SOCKS proxy,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,TestHandler.java,* Tests Handler interface.,1-3,Tests Handler interface.,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,FsConstants.java,* FileSystem related constants.,1-3,FileSystem related constants,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,BlockUtils.java,* Utils functions to help block functions.,1-3,Utils functions to help block functions,,Utils functions to help block functions,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,OpportunisticContainersStatusPBImpl.java,* Protocol Buffer implementation of OpportunisticContainersStatus.,1-3,Protocol Buffer implementation of OpportunisticContainersStatus,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,TestFileSizeCountTask.java,* Unit test for File Size Count Task.,1-3,Unit test for File Size Count Task,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,DockerKillCommand.java,* Encapsulates the docker kill command and its command line arguments.,1-3,Encapsulates the docker kill command and its command line arguments.,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,DfsServlet.java,* A base class for the servlets in DFS.,1-3,A base class for the servlets in DFS,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,LogParserUtil.java,* Common utility functions for {@link LogParser}.,1-3,Common utility functions for {@link LogParser},,,,,,,,,,,,,Common utility functions for {@link LogParser},,,,,,,,,
E2,,E3 (agree),,RSLegacyRawErasureCoderFactory.java,* A raw coder factory for the legacy raw Reed-Solomon coder in Java.,1-3,A raw coder factory for the legacy raw Reed-Solomon coder in Java.,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,InfoKeyHandler.java,* Executes Info Object.,1-3,Executes Info Object,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,TestSeveralNameNodes.java,* Test that we can start several and run with namenodes on the same minicluster,1-3,Test that we can start several and run with namenodes on the same minicluster,we can start several and run with namenodes on the same minicluster,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,TestRegistrySecurityHelper.java,* Test for registry security operations,1-3,Test for registry security operations,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,SequenceFileRecordReader.java,* An {@link RecordReader} for {@link SequenceFile}s.,1-3,* An {@link RecordReader} for {@link SequenceFile}s.,,,,,,,,,,,,,* An {@link RecordReader} for {@link SequenceFile}s.,,,,,,,,,
E2,,E3 (agree),,TestOMVolumeRequest.java,* Base test class for Volume request.,1-3,Base test class for Volume request,,Base test class for Volume request,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,TestSequenceFile.java,Support for flat files of binary key/value pairs.,1-3,Support for flat files of binary key/value pairs,Support for flat files of binary key/value pairs,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,ITestListPerformance.java,* Test list performance.,1-3,Test list performance,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,RegistryOperations.java,* Registry Operations,1-3,Registry Operations,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,LocatedFileStatus.java,* This class defines a FileStatus that includes a file's block locations.,1-3,This class defines a FileStatus that includes a file's block locations,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,IDataLoader.java,* an IDataLoader loads data on demand,1-3,an IDataLoader loads data on demand,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,PartitionResourcesInfo.java,* This class represents queue/user resource usage info for a given partition,1-3,This class represents queue/user resource usage info for a given partition,,This class represents queue/user resource usage info for a given partition,,,,,,,,,,,,,,,,,,,,
E2,,E3 (disagree),,Nfs3Status.java,* Success or error status is reported in NFS3 responses.,1-3,* Success or error status is reported in NFS3 responses.,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,NativeBatchProcessor.java,"* used to create channel, transfer data and command between Java and native",1-3,,,"used to create channel, transfer data and command between Java and native",,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,FederationPolicyException.java,* Generic policy exception.,1-3,Generic policy exception,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,CpuTimeTracker.java,* Utility for sampling and computing CPU usage.,1-3,Utility for sampling and computing CPU usage,,Utility for sampling and computing CPU usage,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,ITestAzureNativeContractSeek.java,* Contract test.,1-3,Contract test,,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,Abfss.java,"* Azure Blob File System implementation of AbstractFileSystem.
* This impl delegates to the old FileSystem",4-4,Azure Blob File System implementation of AbstractFileSystem.,This impl delegates to the old FileSystem,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,ContainerFinishData.java,"* The class contains the fields that can be determined when
* <code>RMContainer</code> finishes, and that need to be stored persistently.",4-4,"The class contains the fields that can be determined when * <code>RMContainer</code> finishes, and that need to be stored persistently.",,"The class contains the fields that can be determined when * <code>RMContainer</code> finishes, and that need to be stored persistently.",,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,TestRollingFileSystemSinkWithLocal.java,"* Test the {@link RollingFileSystemSink} class in the context of the local file
* system.",4-4,Test the {@link RollingFileSystemSink} class in the context of the local file * system.,,,,,,,,,,,,,Test the {@link RollingFileSystemSink} class in the context of the local file * system.,,,,,,,,,
E4,E1 (agree),,,TestTopCLI.java,"* Test class for TopCli.
*",4-4,Test class for TopCli.,,,,,,,,,,,,,,,,,,,,,,
E4,E1 (disagree),,,PlanningQuotaException.java,"* This exception is thrown if the user quota is exceed while accepting or
* updating a reservation.",4-4,This exception is thrown if the user quota is exceed while accepting or * updating a reservation.,,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,TestConfServlet.java,"* Basic test case that the ConfServlet can write configuration
* to its output in XML and JSON format.",4-4,Basic test case that the ConfServlet can write configuration * to its output in XML and JSON format.,Basic test case that the ConfServlet can write configuration * to its output in XML and JSON format.,,,,,,,,,,,,,,,,,,,,,
E4,E1 (disagree),,,SwiftBadRequestException.java,"* Thrown to indicate that data locality can't be calculated or requested path is incorrect.
* Data locality can't be calculated if Openstack Swift version is old.",4-4,Thrown to indicate that data locality can't be calculated or requested path is incorrect.,,,Data locality can't be calculated if Openstack Swift version is old.,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,ByteArrayEncodingState.java,"* A utility class that maintains encoding state during an encode call using
* byte array inputs.",4-4,A utility class that maintains encoding state during an encode call using * byte array inputs.,A utility class that maintains encoding state during an encode call using * byte array inputs.,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,TestTaskContext.java,* Tests context api and {@link StatusReporter#getProgress()} via * {@link TaskAttemptContext#getProgress()} API .,4-4,Tests context api and {@link StatusReporter#getProgress()} via * {@link TaskAttemptContext#getProgress()} API .,,,,,,,,,,,,,Tests context api and {@link StatusReporter#getProgress()} via * {@link TaskAttemptContext#getProgress()} API .,,,,,,,,,
E4,,E3 (agree),,ITestS3Select.java,"* Test the S3 Select feature with some basic SQL Commands.
* Executed if the destination store declares its support for the feature.",4-4,Test the S3 Select feature with some basic SQL Commands.,Executed if the destination store declares its support for the feature.,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,TestDominantResourceFairnessPolicy.java,"* comparator.compare(sched1, sched2) < 0 means that sched1 should get a
* container before sched2",4-4,,"comparator.compare(sched1, sched2) < 0 means that sched1 should get a * container before sched2",,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,Nfs3Metrics.java,"* This class is for maintaining the various NFS gateway activity statistics and
* publishing them through the metrics interfaces.",4-4,This class is for maintaining the various NFS gateway activity statistics and * publishing them through the metrics interfaces.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,RetriableDirectoryCreateCommand.java,"* This class extends Retriable command to implement the creation of directories
* with retries on failure.",4-4,This class extends Retriable command to implement the creation of directories * with retries on failure.,,,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,GetSubClusterPolicyConfigurationRequestPBImpl.java,"* Protocol buffer based implementation of
* {@link GetSubClusterPolicyConfigurationRequest}.",4-4,Protocol buffer based implementation of * {@link GetSubClusterPolicyConfigurationRequest}.,,,,,,,,,,,,,Protocol buffer based implementation of * {@link GetSubClusterPolicyConfigurationRequest}.,,,,,,,,,
E4,,E3 (agree),,ITestS3SelectCLI.java,"* Test the S3 Select CLI through some operations against landsat
* and files generated from it.",4-4,Test the S3 Select CLI through some operations against landsat * and files generated from it.,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,Expression.java,"* Interface describing an expression to be used in the
* {@link org.apache.hadoop.fs.shell.find.Find} command.",4-4,Interface describing an expression to be used in the * {@link org.apache.hadoop.fs.shell.find.Find} command.,,to be used in the * {@link org.apache.hadoop.fs.shell.find.Find} command.,,,,,,,,,,,to be used in the * {@link org.apache.hadoop.fs.shell.find.Find} command.,,,,,,,,,
E2,E1 (disagree),,,DFSConfigKeys.java,"* This class contains constants for configuration keys and default values
* used in hdfs.",4-4,This class contains constants for configuration keys and default values used in hdfs.,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,AbstractTFLaunchCommandTestHelper.java,"* This class is an abstract base class for testing Tensorboard and TensorFlow
* launch commands.",4-4,This class is an abstract base class for testing Tensorboard and TensorFlow and launch commands.,,This class is an abstract base class for testing Tensorboard and TensorFlow and launch commands.,,,,,,,,,,,,,,,,,,,,
E2,E1 (disagree),,,DeleteApplicationHomeSubClusterRequest.java,"* The request to <code>Federation state store</code> to delete the mapping of
* home subcluster of a submitted application.",4-4,The request to <code>Federation state store</code> to delete the mapping of * home subcluster of a submitted application.,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,PartialOutputCommitter.java,"* Interface for an {@link org.apache.hadoop.mapreduce.OutputCommitter}
* implementing partial commit of task output, as during preemption.",4-4,"* Interface for an {@link org.apache.hadoop.mapreduce.OutputCommitter} * implementing partial commit of task output, as during preemption.",,,,,,,,,,,,,Interface for an {@link org.apache.hadoop.mapreduce.OutputCommitter},,,,,,,,,
E2,E1 (agree),,,NodeUpdateType.java,"* <p>Taxonomy of the <code>NodeState</code> that a
* <code>Node</code> might transition into.</p>
*",4-4,<p>Taxonomy of the <code>NodeState</code> that a * <code>Node</code> might transition into.</p>,<p>Taxonomy of the <code>NodeState</code> that a * <code>Node</code> might transition into.</p>,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,BlockPlacementStatusWithNodeGroup.java,"* An implementation of @see BlockPlacementStatus for
* @see BlockPlacementPolicyWithNodeGroup",4-4,An implementation of @see BlockPlacementStatus,,for * @see BlockPlacementPolicyWithNodeGroup,,,,,,,,,,,for * @see BlockPlacementPolicyWithNodeGroup,,,,,,,,,
E2,E1 (agree),,,AutoInputFormat.java,"* An {@link InputFormat} that tries to deduce the types of the input files
* automatically. It can currently handle text and sequence files.",4-4,An {@link InputFormat} that tries to deduce the types of the input files * automatically.,It can currently handle text and sequence files.,,,,,,,,,,,,An {@link InputFormat} that tries to deduce the types of the input files * automatically.,,,,,,,,,
E2,,E3 (agree),,GetSafeModeRequestPBImpl.java,"* Protobuf implementation of the state store API object
* GetSafeModeRequest.",4-4,Protobuf implementation of the state store API object * GetSafeModeRequest.,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,TestConnCache.java,"* This class tests the client connection caching in a single node
* mini-cluster.",4-4,This class tests the client connection,caching in a single node * mini-cluster.,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,Node2ObjectsMap.java,"* This data structure maintains the list of containers that is on a datanode.
* This information is built from the DN container reports.",4-4,This data structure maintains the list of containers that is on a datanode.,This information is built from the DN container reports.,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,DFSConfigKeys.java,"* This class contains constants for configuration keys and default values
* used in hdfs.",4-4,This class contains constants for configuration keys and default values used in hdfs.,,This class contains constants for configuration keys and default values used in hdfs.,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,ApplicationStateData.java,* Contains all the state data that needs to be stored persistently * for an Application,4-4,Contains all the state data that needs to be stored persistently * for an Application,,needs to be stored persistently * for an Application,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,GetApplicationHomeSubClusterResponsePBImpl.java,"* Protocol buffer based implementation of
* {@link GetApplicationHomeSubClusterResponse}.",4-4,Protocol buffer based implementation of * {@link GetApplicationHomeSubClusterResponse}.,,,,,,,,,,,,,Protocol buffer based implementation of * {@link GetApplicationHomeSubClusterResponse}.,,,,,,,,,
E2,,E3 (agree),,InvalidContainerRequestException.java,"* Thrown when an arguments are combined to construct a
* <code>AMRMClient.ContainerRequest</code> in an invalid way.",4-4,* Thrown when an arguments are combined to construct a * <code>AMRMClient.ContainerRequest</code> in an invalid way.,,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,FederationProtocolPBTranslator.java,"* Helper class for setting/getting data elements in an object backed by a
* protobuf implementation.",4-4,Helper class for setting/getting data elements in an object backed by a * protobuf implementation.,,for setting/getting data elements in an object backed by a * protobuf implementation.,,,* Thrown when an arguments are combined to construct a * <code>AMRMClient.ContainerRequest</code> in an invalid way.,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,StateStoreFileSystemImpl.java,"* {@link StateStoreDriver} implementation based on a filesystem. The common
* implementation uses HDFS as a backend. The path can be specified setting
* dfs.federation.router.driver.fs.path=hdfs://host:port/path/to/store.",5-7,* {@link StateStoreDriver} implementation based on a filesystem.,The common * implementation uses HDFS as a backend.,,,The path can be specified setting * dfs.federation.router.driver.fs.path=hdfs://host:port/path/to/store.,,,,,,,,,* {@link StateStoreDriver} implementation based on a filesystem.,,,,,,,,,
E4,E1 (agree),,,RandomKeyGenerator.java,"* Data generator tool to generate as much keys as possible.
|
* Wrapper to hold ozone keyValidate entry.
|
* Validates the write done in ozone cluster.",5-7,Data generator tool to generate as much keys as possible.,"Wrapper to hold ozone keyValidate entry.
|
* Validates the write done in ozone cluster.",,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,ShellBasedUnixGroupsMapping.java,"* A simple shell-based implementation of {@link GroupMappingServiceProvider} * that exec's the <code>groups</code> shell command to fetch the group
* memberships of a given user.",5-7,A simple shell-based implementation of {@link GroupMappingServiceProvider},that exec's the <code>groups</code> shell command to fetch the group * memberships of a given user.,,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,ITestCommitOperations.java,"* Test the low-level binding of the S3A FS to the magic commit mechanism,
* and handling of the commit operations.
* This is done with an inconsistent client.",5-7,"* Test the low-level binding of the S3A FS to the magic commit mechanism, * and handling of the commit operations.",,,,This is done with an inconsistent client.,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,TestNativeAzureFileSystemUploadLogic.java,"* Tests for the upload, buffering and flush logic in WASB.
| Just an arbitrary number so that the values I write have a predictable|
* Various scenarios to test in how often we flush data while uploading.",5-7,"Tests for the upload, buffering and flush logic in WASB.",| Just an arbitrary number so that the values I write have a predictable|,* Various scenarios to test in how often we flush data while uploading.,,,,,,,,,,,,,,,,,,,,
E4,E1 (disagree),,,ITestAbfsReadWriteAndSeek.java,"* Test read, write and seek.
* Uses package-private methods in AbfsConfiguration, which is why it is in
* this package.",5-7,"Test read, write and seek.",,"Uses package-private methods in AbfsConfiguration, which is why it is in * this package.",,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,TestProtoBufRpc.java,"* Test for testing protocol buffer based RPC mechanism.
* This test depends on test.proto definition of types in src/test/proto
* and protobuf service definition from src/test/test_rpc_service.proto",5-7,Test for testing protocol buffer based RPC mechanism.,"This test depends on test.proto definition of types in src/test/proto
* and protobuf service definition from src/test/test_rpc_service.proto",,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,NativeSingleLineParser.java,"* This sample parser will parse the sample log and extract the resource
* skyline.
* <p> The expected log format is: NormalizedJobName NumInstances SubmitTime
* StartTime EndTime JobInstanceName memUsage coreUsage",5-7,"This sample parser will parse the sample log and extract the resource
* skyline.","<p> The expected log format is: NormalizedJobName NumInstances SubmitTime
* StartTime EndTime JobInstanceName memUsage coreUsage",,,,,,,,,,,,,,,,,,,,,
E4,,E3 (disagree),,ErasureCodingPolicyManager.java,"* This manages erasure coding policies predefined and activated in the system.
* It loads customized policies and syncs with persisted ones in
* NameNode image.
*
* This class is instantiated by the FSNamesystem.",5-7,This manages erasure coding policies predefined and activated in the system,"It loads customized policies and syncs with persisted ones in
* NameNode image.",,,This class is instantiated by the FSNamesystem.,,,,,,,,,* This class is instantiated by the FSNamesystem.,,,,,,,,,
E4,,E3 (disagree),,OMNodeDetails.java,"* This class stores OM node details.
|
* Builder class for OMNodeDetails.",5-7,"* This class stores OM node details.
|
* Builder class for OMNodeDetails.",,,,,,,,,,,,,* Builder class for OMNodeDetails.,,,,,,,,,
E4,,E3 (agree),,YarnAuthorizationProvider.java,"* An implementation of the interface will provide authorization related
* information and enforce permission check. It is excepted that any of the
* methods defined in this interface should be non-blocking call and should not
* involve expensive computation as these method could be invoked in RPC.",5-7,"An implementation of the interface will provide authorization related
* information and enforce permission check.","It is excepted that any of the
* methods defined in this interface should be non-blocking call and should not
* involve expensive computation as these method could be invoked in RPC.","It is excepted that any of the
* methods defined in this interface should be non-blocking call and should not
* involve expensive computation as these method could be invoked in RPC.",,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,BatchedRequests.java,"* A grouping of Scheduling Requests which are sent to the PlacementAlgorithm
* to place as a batch. The placement algorithm tends to give more optimal
* placements if more requests are batched together.
| PlacementAlgorithmOutput attempt - the number of times the requests in this|
* Iterator Type.",5-7,"A grouping of Scheduling Requests which are sent to the PlacementAlgorithm
* to place as a batch
Iterator Type.","The placement algorithm tends to give more optimal
* placements if more requests are batched together.",,,| PlacementAlgorithmOutput attempt - the number of times the requests in this|,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,InconsistentS3ClientFactory.java,"* S3 Client factory used for testing with eventual consistency fault injection.
* This client is for testing <i>only</i>; it is in the production
* {@code hadoop-aws} module to enable integration tests to use this
* just by editing the Hadoop configuration used to bring up the client.",5-7,S3 Client factory used for testing with eventual consistency fault injection.,,,,"This client is for testing <i>only</i>; it is in the production
* {@code hadoop-aws} module to enable integration tests to use this
* just by editing the Hadoop configuration used to bring up the client.",,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,UniformSizeInputFormat.java,"* UniformSizeInputFormat extends the InputFormat class, to produce
* input-splits for DistCp.
* It looks at the copy-listing and groups the contents into input-splits such
* that the total-number of bytes to be copied for each input split is
* uniform.",5-7,"UniformSizeInputFormat extends the InputFormat class, to produce
* input-splits for DistCp.","It looks at the copy-listing and groups the contents into input-splits such
* that the total-number of bytes to be copied for each input split is
* uniform.",,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,BlocksMap.java,"* This class maintains the map from a block to its metadata.
* block's metadata currently includes blockCollection it belongs to and
* the datanodes that store the block.",5-7,This class maintains the map from a block to its metadata.,"block's metadata currently includes blockCollection it belongs to and
* the datanodes that store the block.",,,,,,,,,,,,,,,,,,,,,
E4,,E3 (disagree),,SafeModeException.java,"* This exception is thrown when the name node is in safe mode.
* Client cannot modified namespace until the safe mode is off.
*",5-7,,,,,Client cannot modified namespace until the safe mode is off.,* This exception is thrown when the name node is in safe mode.,,,,,,,,,,,,,* Client cannot modified namespace until the safe mode is off.,,,,
E2,E1 (agree),,,SchedulerQueueManager.java,"*
* Context of the Queues in Scheduler.
*",5-7,Context of the Queues in Scheduler.,,,,,,,,,,,,,,,,,,,,,,
E2,E1 (disagree),,,MetricsCache.java,"* A metrics cache for sinks that don't support sparse updates.
|
* Cached record",5-7,A metrics cache for sinks that don't support sparse updates. Context of the Queues in Scheduler.,Cached record,,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,TypedEvent.java,"* Basic event implementation to implement custom events.
*
* @param <T>",5-7,Basic event implementation,,to implement custom events.,,@param <T>,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,TestCustomOauthTokenProvider.java,"* Test custom OAuth token providers.
* This is a unit test not an E2E integration test because that would
* require OAuth auth setup, always.
* Instead this just checks that the creation works and that everything
* is propagated.",5-7,Test custom OAuth token providers.,"Instead this just checks that the creation works and that everything
* is propagated.","This is a unit test not an E2E integration test because that would * require OAuth auth setup, always.",,,,,,,,,,,,,,,,,,,,
E2,E1 (disagree),,,RMAdminRequestInterceptor.java,"* Defines the contract to be implemented by the request intercepter classes,
* that can be used to intercept and inspect messages sent from the client to
* the resource manager.",5-7,Defines the contract to be implemented by the request intercepter classes,,,,that can be used to intercept and inspect messages sent from the client to * the resource manager.,,,,,,,,,,,,,,,,,,
E2,E1 (disagree),,,BaseRouterWebServicesTest.java,"* Base class for all the RouterRMAdminService test cases. It provides utility
* methods that can be used by the concrete test case classes.
*",5-7,* Base class for all the RouterRMAdminService test cases.,,,,It provides utility * methods that can be used by the concrete test case classes.,,,,,,,,,,,,,,,,,,
E2,E1 (disagree),,,StreamBaseRecordReader.java,"* Shared functionality for hadoopStreaming formats. A custom reader can be
* defined to be a RecordReader with the constructor below and is selected with
* the option bin/hadoopStreaming -inputreader ...
* * @see StreamXmlRecordReader",5-7,Shared functionality for hadoopStreaming formats.,,,,"A custom reader can be
* defined to be a RecordReader with the constructor below and is selected with
* the option bin/hadoopStreaming -inputreader ...",,,,,,,,,A custom reader can be * defined to be a RecordReader with the constructor below and is selected with * the option bin/hadoopStreaming -inputreader ... * * @see StreamXmlRecordReader,,,,,,,,,
E2,,E3 (disagree),,MapContext.java,"* The context that is given to the {@link Mapper}.
* @param <KEYIN> the key input type to the Mapper
* @param <VALUEIN> the value input type to the Mapper
* @param <KEYOUT> the key output type from the Mapper
* @param <VALUEOUT> the value output type from the Mapper",5-7,The context that is given to the {@link Mapper}.,,,,"* @param <KEYIN> the key input type to the Mapper
* @param <VALUEIN> the value input type to the Mapper
* @param <KEYOUT> the key output type from the Mapper
* @param <VALUEOUT> the value output type from the Mapper",,,,,,,,,* The context that is given to the {@link Mapper}. * @param <KEYIN> the key input type to the Mapper * @param <VALUEIN> the value input type to the Mapper * @param <KEYOUT> the key output type from the Mapper * @param <VALUEOUT> the value output type from the Mapper,,,,,,,,,
E2,,E3 (agree),,RegistryInternalConstants.java,"* Internal constants for the registry.
*
* These are the things which aren't visible to users.
*",5-7,Internal constants for the registry.,,These are the things which aren't visible to users.,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,Query.java,"* Check if a record matches a query. The query is usually a partial record.
*
* @param <T> Type of the record to query.",5-7,Check if a record matches a query.,The query is usually a partial record.,,,,,,,,,,,,* @param <T> Type of the record to query.,,,,,,,,,
E2,,E3 (agree),,OzoneObj.java,"* Class representing an unique ozone object.
* |
* Ozone Objects supported for ACL.
|
* Ozone Objects supported for ACL.",5-7,Class representing an unique ozone object,Ozone Objects supported for ACL.,,,,,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,TextInputFormat.java,"* An {@link InputFormat} for plain text files. Files are broken into lines.
* Either linefeed or carriage-return are used to signal end of line. Keys are
* the position in the file, and values are the line of text..",5-7,An {@link InputFormat} for plain text files. Files are broken into lines.,"Either linefeed or carriage-return are used to signal end of line. Keys are * the position in the file, and values are the line of text..",,,,,,,,,,,,An {@link InputFormat} for plain text files. Files are broken into lines.,,,,,,,,,
E2,,E3 (agree),,HadoopIllegalArgumentException.java,"* Indicates that a method has been passed illegal or invalid argument. This
* exception is thrown instead of IllegalArgumentException to differentiate the
* exception thrown in Hadoop implementation from the one thrown in JDK.",5-7,Indicates that a method has been passed illegal or invalid argument.,This * exception is thrown instead of IllegalArgumentException to differentiate the * exception thrown in Hadoop implementation from the one thrown in JDK.,,,,This * exception is thrown instead of IllegalArgumentException to differentiate the * exception thrown in Hadoop implementation from the one thrown in JDK.,,,,,,,,,,,,,,,,,
E2,,E3 (agree),,DistributedSchedulingAllocateRequest.java,"* Object used by the Application Master when distributed scheduling is enabled,
* in order to forward the {@link AllocateRequest} for GUARANTEED containers to
* the Resource Manager, and to notify the Resource Manager about the allocation
* of OPPORTUNISTIC containers through the Distributed Scheduler.",5-7,"* Object used by the Application Master when distributed scheduling is enabled,","in order to forward the {@link AllocateRequest} for GUARANTEED containers to * the Resource Manager, and to notify the Resource Manager about the allocation * of OPPORTUNISTIC containers through the Distributed Scheduler.",,,,,,,,,,,,"in order to forward the {@link AllocateRequest} for GUARANTEED containers to * the Resource Manager, and to notify the Resource Manager about the allocation * of OPPORTUNISTIC containers through the Distributed Scheduler.",,,,,,,,,
E2,,E3 (agree),,FederationStateStoreInvalidInputException.java,"* Exception thrown by the {@code FederationMembershipStateStoreInputValidator},
* {@code FederationApplicationHomeSubClusterStoreInputValidator},
* {@code FederationPolicyStoreInputValidator} if the input is invalid.
*",5-7,"Exception thrown by the {@code FederationMembershipStateStoreInputValidator},",,,,,"* Exception thrown by the {@code FederationMembershipStateStoreInputValidator}, * {@code FederationApplicationHomeSubClusterStoreInputValidator}, * {@code FederationPolicyStoreInputValidator} if the input is invalid.",,,,,,,,"* Exception thrown by the {@code FederationMembershipStateStoreInputValidator}, * {@code FederationApplicationHomeSubClusterStoreInputValidator}, * {@code FederationPolicyStoreInputValidator} if the input is invalid.",,,,,,,,,
E4,E1 (agree),,,TestUpgradeDomainBlockPlacementPolicy.java,"* End-to-end test case for upgrade domain
* The test configs upgrade domain for nodes via admin json
* config file and put some nodes to decommission state.
* The test then verifies replicas are placed on the nodes that
* satisfy the upgrade domain policy.
*",8-375,End-to-end test case for upgrade domain,"The test configs upgrade domain for nodes via admin json
* config file and put some nodes to decommission state.
* The test then verifies replicas are placed on the nodes that
* satisfy the upgrade domain policy.",,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,ResourceRequestsJsonVerifications.java,"* Performs value verifications on
* {@link org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceRequestInfo}
* objects against the values of {@link ResourceRequest}. With the help of the
* {@link Builder}, users can also make verifications of the custom resource
* types and its values.
|
* Builder class for {@link ResourceRequestsJsonVerifications}.",8-375,"* Performs value verifications on
* {@link org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceRequestInfo}
* objects against the values of {@link ResourceRequest}.
|
* Builder class for {@link ResourceRequestsJsonVerifications}.","With the help of the
* {@link Builder}, users can also make verifications of the custom resource
* types and its values.",,,,,,,,,,,,"* Performs value verifications on
* {@link org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceRequestInfo}
* objects against the values of {@link ResourceRequest}. With the help of the
* {@link Builder}, users can also make verifications of the custom resource
* types and its values.
|
* Builder class for {@link ResourceRequestsJsonVerifications}.",,,,,,,,,
E4,E1 (agree),,,EditLogTailer.java,"* EditLogTailer represents a thread which periodically reads from edits
* journals and applies the transactions contained within to a given
* FSNamesystem.
|
* The thread which does the actual work of tailing edits journals and
* applying the transactions to the FSNS.
|
* Manage the 'active namenode proxy'. This cannot just be the a single proxy since we could
* failover across a number of NameNodes, rather than just between an active and a standby.
* <p>
* We - lazily - get a proxy to one of the configured namenodes and attempt to make the request
* against it. If it doesn't succeed, either because the proxy failed to be created or the request
* failed, we try the next NN in the list. We try this up to the configuration maximum number of
* retries before throwing up our hands. A working proxy is retained across attempts since we
* expect the active NameNode to switch rarely.
* <p>
* This mechanism is <b>very bad</b> for cases where we care about being <i>fast</i>; it just
* blindly goes and tries namenodes.",8-375,"EditLogTailer represents a thread which periodically reads from edits
* journals and applies the transactions contained within to a given
* FSNamesystem.","|
* The thread which does the actual work of tailing edits journals and
* applying the transactions to the FSNS.
|","* Manage the 'active namenode proxy'. This cannot just be the a single proxy since we could
* failover across a number of NameNodes, rather than just between an active and a standby.
* <p>
* We - lazily - get a proxy to one of the configured namenodes and attempt to make the request
* against it. If it doesn't succeed, either because the proxy failed to be created or the request
* failed, we try the next NN in the list. We try this up to the configuration maximum number of
* retries before throwing up our hands. A working proxy is retained across attempts since we
* expect the active NameNode to switch rarely.
* <p>
* This mechanism is <b>very bad</b> for cases where we care about being <i>fast</i>; it just
* blindly goes and tries namenodes.",,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,TestSaveNamespace.java,"* Test various failure scenarios during saveNamespace() operation.
* Cases covered:
* <ol>
* <li>Recover from failure while saving into the second storage directory</li>
* <li>Recover from failure while moving current into lastcheckpoint.tmp</li>
* <li>Recover from failure while moving lastcheckpoint.tmp into
* previous.checkpoint</li>
* <li>Recover from failure while rolling edits file</li>
* </ol>",8-375,Test various failure scenarios during saveNamespace() operation.,"Cases covered:
* <ol>
* <li>Recover from failure while saving into the second storage directory</li>
* <li>Recover from failure while moving current into lastcheckpoint.tmp</li>
* <li>Recover from failure while moving lastcheckpoint.tmp into
* previous.checkpoint</li>
* <li>Recover from failure while rolling edits file</li>
* </ol>",,,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,KerberosDelegationTokenAuthenticator.java,"* The <code>KerberosDelegationTokenAuthenticator</code> provides support for
* Kerberos SPNEGO authentication mechanism and support for Hadoop Delegation
* Token operations.
* <p>
* It falls back to the {@link PseudoDelegationTokenAuthenticator} if the HTTP
* endpoint does not trigger a SPNEGO authentication",8-375,"The <code>KerberosDelegationTokenAuthenticator</code> provides support for
* Kerberos SPNEGO authentication mechanism and support for Hadoop Delegation
* Token operations.","It falls back to the {@link PseudoDelegationTokenAuthenticator} if the HTTP
* endpoint does not trigger a SPNEGO authentication",,,,,,,,,,,,"It falls back to the {@link PseudoDelegationTokenAuthenticator} if the HTTP
* endpoint does not trigger a SPNEGO authentication",,,,,,,,,
E4,E1 (agree),,,CapacitySchedulerPlanFollower.java,"* This class implements a {@link PlanFollower}. This is invoked on a timer, and
* it is in charge to publish the state of the {@link Plan}s to the underlying
* {@link CapacityScheduler}. This implementation does so, by
* adding/removing/resizing leaf queues in the scheduler, thus affecting the
* dynamic behavior of the scheduler in a way that is consistent with the
* content of the plan. It also updates the plan's view on how much resources
* are available in the cluster.
* * This implementation of PlanFollower is relatively stateless, and it can
* synchronize schedulers and Plans that have arbitrary changes (performing set
* differences among existing queues). This makes it resilient to frequency of
* synchronization, and RM restart issues (no ""catch up"" is necessary).",8-375,"This class implements a {@link PlanFollower}. This is invoked on a timer, and
* it is in charge to publish the state of the {@link Plan}s to the underlying
* {@link CapacityScheduler}.","This implementation does so, by
* adding/removing/resizing leaf queues in the scheduler, thus affecting the
* dynamic behavior of the scheduler in a way that is consistent with the
* content of the plan. It also updates the plan's view on how much resources
* are available in the cluster.","This implementation of PlanFollower is relatively stateless, and it can
* synchronize schedulers and Plans that have arbitrary changes (performing set
* differences among existing queues). This makes it resilient to frequency of
* synchronization, and RM restart issues (no ""catch up"" is necessary).",,,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,TestTaskCommit.java,"* Special Committer that does not cleanup temporary files in
* abortTask
* * The framework's FileOutputCommitter cleans up any temporary
* files left behind in abortTask. We want the test case to
* find these files and hence short-circuit abortTask.
|
* Special committer that always requires commit.",8-375,"Special Committer that does not cleanup temporary files in
* abortTask",,"The framework's FileOutputCommitter cleans up any temporary
* files left behind in abortTask. We want the test case to
* find these files and hence short-circuit abortTask.",,* Special committer that always requires commit.,,,,,,,,,,,,,,,,,,
E4,E1 (agree),,,LoggedTask.java,"* A {@link LoggedTask} represents a [hadoop] task that is part of a hadoop job.
* It knows about the [pssibly empty] sequence of attempts, its I/O footprint,
* and its runtime.
* * All of the public methods are simply accessors for the instance variables we
* want to write out in the JSON files.
*",8-375,A {@link LoggedTask} represents a [hadoop] task that is part of a hadoop job.,"It knows about the [pssibly empty] sequence of attempts, its I/O footprint,
* and its runtime.",,,"All of the public methods are simply accessors for the instance variables we
* want to write out in the JSON files.
*",,,,,,,,,A {@link LoggedTask} represents a [hadoop] task that is part of a hadoop job.,,,,,,,,,
E4,E1 (agree),,,AbstractContractGetFileStatusTest.java,"* Test getFileStatus and related listing operations.
| the tree parameters. Kept small to avoid killing object store test| runs too much.|
* Accept everything.
| the tree parameters. Kept small to avoid killing object store test| runs too much.|
* Accept nothing.
| the tree parameters. Kept small to avoid killing object store test| runs too much.|
* Path filter which only expects paths whose final name element
* equals the {@code match} field.
| the tree parameters. Kept small to avoid killing object store test| runs too much.|
* A filesystem filter which exposes the protected method
* {@link #listLocatedStatus(Path, PathFilter)}.",8-375,Test getFileStatus and related listing operations.,"| the tree parameters. Kept small to avoid killing object store test| runs too much.|
* Accept everything.
| the tree parameters. Kept small to avoid killing object store test| runs too much.|
* Accept nothing.
| the tree parameters. Kept small to avoid killing object store test| runs too much.|
* Path filter which only expects paths whose final name element
* equals the {@code match} field.
| the tree parameters. Kept small to avoid killing object store test| runs too much.|
* A filesystem filter which exposes the protected method
* {@link #listLocatedStatus(Path, PathFilter)}.",,,,,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,FairOrderingPolicy.java,"* An OrderingPolicy which orders SchedulableEntities for fairness (see
* FairScheduler
* FairSharePolicy), generally, processes with lesser usage are lesser. If
* sizedBasedWeight is set to true then an application with high demand
* may be prioritized ahead of an application with less usage. This
* is to offset the tendency to favor small apps, which could result in
* starvation for large apps if many small ones enter and leave the queue
* continuously (optional, default false)",8-375,"* An OrderingPolicy which orders SchedulableEntities for fairness (see
* FairScheduler
* FairSharePolicy), generally, processes with lesser usage are lesser",,,,"If
* sizedBasedWeight is set to true then an application with high demand
* may be prioritized ahead of an application with less usage. This
* is to offset the tendency to favor small apps, which could result in
* starvation for large apps if many small ones enter and leave the queue
* continuously (optional, default false)",,,,,,,,,"* An OrderingPolicy which orders SchedulableEntities for fairness (see
* FairScheduler
* FairSharePolicy), generally, processes with lesser usage are lesser",,,,,,,,,
E4,,E3 (disagree),,WritableComparator.java,"A Comparator for {@link WritableComparable}s.
*
* <p>This base implementation uses the natural ordering. To define alternate
* orderings, override {@link #compare(WritableComparable,WritableComparable)}.
*
* <p>One may optimize compare-intensive operations by overriding
* {@link #compare(byte[],int,int,byte[],int,int)}. Static utility methods are
* provided to assist in optimized implementations of this method.",8-375,A Comparator for {@link WritableComparable}s.,"* <p>This base implementation uses the natural ordering. To define alternate
* orderings, override {@link #compare(WritableComparable,WritableComparable)}.",,,"<p>This base implementation uses the natural ordering. To define alternate
* orderings, override {@link #compare(WritableComparable,WritableComparable)}.
*
* <p>One may optimize compare-intensive operations by overriding
* {@link #compare(byte[],int,int,byte[],int,int)}. Static utility methods are
* provided to assist in optimized implementations of this method.",,,,,,,,,"A Comparator for {@link WritableComparable}s.
*
* <p>This base implementation uses the natural ordering. To define alternate
* orderings, override {@link #compare(WritableComparable,WritableComparable)}.
*
* <p>One may optimize compare-intensive operations by overriding
* {@link #compare(byte[],int,int,byte[],int,int)}. Static utility methods are
* provided to assist in optimized implementations of this method.",,,,,,,,,
E4,,E3 (disagree),,TestHDFSServerPorts.java,"* This test checks correctness of port usage by hdfs components:
* NameNode, DataNode, SecondaryNamenode and BackupNode.
* * The correct behavior is:<br> * - when a specific port is provided the server must either start on that port * or fail by throwing {@link java.net.BindException}.<br>
* - if the port = 0 (ephemeral) then the server should choose * a free port and start on it.",8-375,"* This test checks correctness of port usage by hdfs components:
* NameNode, DataNode, SecondaryNamenode and BackupNode.","* * The correct behavior is:<br> * - when a specific port is provided the server must either start on that port * or fail by throwing {@link java.net.BindException}.<br>
* - if the port = 0 (ephemeral) then the server should choose * a free port and start on it.",,,,* * The correct behavior is:<br> * - when a specific port is provided the server must either start on that port * or fail by throwing {@link java.net.BindException},,,,,,,,,,,,,,,,,
E4,,E3 (disagree),,TestSwiftFileSystemContract.java,"* This is the full filesystem contract test -which requires the
* Default config set up to point to a filesystem.
*
* Some of the tests override the base class tests -these
* are where SwiftFS does not implement those features, or
* when the behavior of SwiftFS does not match the normal
* contract -which normally means that directories and equal files
* are being treated as equal.",8-375,"* This is the full filesystem contract test -which requires the
* Default config set up to point to a filesystem.",,"* Some of the tests override the base class tests -these
* are where SwiftFS does not implement those features, or
* when the behavior of SwiftFS does not match the normal
* contract -which normally means that directories and equal files
* are being treated as equal.",,,,,,,,,,,,,,,,,,,"* Some of the tests override the base class tests -these
* are where SwiftFS does not implement those features, or
* when the behavior of SwiftFS does not match the normal
* contract -which normally means that directories and equal files
* are being treated as equal.",
E4,,E3 (agree),,DBProfile.java,"* User visible configs based RocksDB tuning page. Documentation for Options.
* <p>
* https://github.com/facebook/rocksdb/blob/master/include/rocksdb/options.h
* <p>
* Most tuning parameters are based on this URL.
* <p>
* https://github.com/facebook/rocksdb/wiki/Setup-Options-and-Basic-Tuning",8-375,* User visible configs based RocksDB tuning page.,,,,"Documentation for Options.
* <p>
* https://github.com/facebook/rocksdb/blob/master/include/rocksdb/options.h
* <p>
* Most tuning parameters are based on this URL.
* <p>
* https://github.com/facebook/rocksdb/wiki/Setup-Options-and-Basic-Tuning",,,,,,,,,"Documentation for Options.
* <p>
* https://github.com/facebook/rocksdb/blob/master/include/rocksdb/options.h
* <p>
* Most tuning parameters are based on this URL.
* <p>
* https://github.com/facebook/rocksdb/wiki/Setup-Options-and-Basic-Tuning",,,,,,,,,
E4,,E3 (disagree),,DelegatingSSLSocketFactory.java,"* A {@link SSLSocketFactory} that can delegate to various SSL implementations.
* Specifically, either OpenSSL or JSSE can be used. OpenSSL offers better
* performance than JSSE and is made available via the
* <a href=""https://github.com/wildfly/wildfly-openssl"">wildlfy-openssl</a>
* library.
*
* <p>
* The factory has several different modes of operation:
* <ul>
* <li>OpenSSL: Uses the wildly-openssl library to delegate to the
* system installed OpenSSL. If the wildfly-openssl integration is not
* properly setup, an exception is thrown.</li>
* <li>Default: Attempts to use the OpenSSL mode, if it cannot load the
* necessary libraries, it falls back to the Default_JSEE mode.</li>
* <li>Default_JSSE: Delegates to the JSSE implementation of SSL, but
* it disables the GCM cipher when running on Java 8.</li>
* <li>Default_JSSE_with_GCM: Delegates to the JSSE implementation of
* SSL with no modification to the list of enabled ciphers.</li>
* </ul>
* </p>
| This should only be modified within the #initializeDefaultFactory|
* Default indicates Ordered, preferred OpenSSL, if failed to load then fall
* back to Default_JSSE.
*
* <p>
* Default_JSSE is not truly the the default JSSE implementation because
* the GCM cipher is disabled when running on Java 8. However, the name
* was not changed in order to preserve backwards compatibility. Instead,
* a new mode called Default_JSSE_with_GCM delegates to the default JSSE
* implementation with no changes to the list of enabled ciphers.
* </p>",8-375,* A {@link SSLSocketFactory} that can delegate to various SSL implementations.,"Specifically, either OpenSSL or JSSE can be used. OpenSSL offers better
* performance than JSSE and is made available via the
* <a href=""https://github.com/wildfly/wildfly-openssl"">wildlfy-openssl</a>
* library.","<p>
* Default_JSSE is not truly the the default JSSE implementation because
* the GCM cipher is disabled when running on Java 8. However, the name
* was not changed in order to preserve backwards compatibility. Instead,
* a new mode called Default_JSSE_with_GCM delegates to the default JSSE
* implementation with no changes to the list of enabled ciphers.
* </p>",,"* <p>
* The factory has several different modes of operation:
* <ul>
* <li>OpenSSL: Uses the wildly-openssl library to delegate to the
* system installed OpenSSL. If the wildfly-openssl integration is not
* properly setup, an exception is thrown.</li>
* <li>Default: Attempts to use the OpenSSL mode, if it cannot load the
* necessary libraries, it falls back to the Default_JSEE mode.</li>
* <li>Default_JSSE: Delegates to the JSSE implementation of SSL, but
* it disables the GCM cipher when running on Java 8.</li>
* <li>Default_JSSE_with_GCM: Delegates to the JSSE implementation of
* SSL with no modification to the list of enabled ciphers.</li>
* </ul>
* </p>
| This should only be modified within the #initializeDefaultFactory|
* Default indicates Ordered, preferred OpenSSL, if failed to load then fall
* back to Default_JSSE.",,,,,,,,,"* A {@link SSLSocketFactory} that can delegate to various SSL implementations.
OpenSSL offers better
* performance than JSSE and is made available via the
* <a href=""https://github.com/wildfly/wildfly-openssl"">wildlfy-openssl</a>
* library.",,,| This should only be modified within the #initializeDefaultFactory|,,,,,,
E4,,E3 (disagree),,Utils.java,"* A utility class. It provides
* A path filter utility to filter out output/part files in the output dir
|
* This class filters output(part) files from the given directory
* It does not accept files with filenames _logs and _SUCCESS.
* This can be used to list paths of output directory as follows:
* Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,
* new OutputFilesFilter()));
|
* This class filters log files from directory given
* It doesnt accept paths having _logs.
* This can be used to list paths of output directory as follows:
* Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,
* new OutputLogFilter()));",8-375,"* A utility class. It provides
* A path filter utility to filter out output/part files in the output dir
|","* This class filters output(part) files from the given directory
* It does not accept files with filenames _logs and _SUCCESS.
|
* This class filters log files from directory given
* It doesnt accept paths having _logs.",,,"|
* This class filters output(part) files from the given directory
* It does not accept files with filenames _logs and _SUCCESS.
* This can be used to list paths of output directory as follows:
* Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,
* new OutputFilesFilter()));
|
* This class filters log files from directory given
* It doesnt accept paths having _logs.
* This can be used to list paths of output directory as follows:
* Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,
* new OutputLogFilter()));",,,,,,,,,,,,,,,,,,
E4,,E3 (agree),,ResourceBlacklistRequest.java,"* {@link ResourceBlacklistRequest} encapsulates the list of resource-names * which should be added or removed from the <em>blacklist</em> of resources * for the application.
* * @see ResourceRequest
* @see ApplicationMasterProtocol#allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)",8-375,* {@link ResourceBlacklistRequest} encapsulates the list of resource-names * which should be added or removed from the <em>blacklist</em> of resources * for the application.,,,,,,,,,,,,,"* {@link ResourceBlacklistRequest} encapsulates the list of resource-names * which should be added or removed from the <em>blacklist</em> of resources * for the application.
* * @see ResourceRequest
* @see ApplicationMasterProtocol#allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)",,,,,,,,,
E2,E1 (agree),,,ReencryptionUpdater.java,"* Class for finalizing re-encrypt EDEK operations, by updating file xattrs with
* edeks returned from reencryption.
* <p>
* The tasks are submitted by ReencryptionHandler.
* <p>
* It is assumed only 1 Updater will be running, since updating file xattrs
* requires namespace write lock, and performance gain from multi-threading
* is limited.
|
* Class to track re-encryption submissions of a single zone. It contains
* all the submitted futures, and statistics about how far the futures are
* processed.
|
* Class representing the task for one batch of a re-encryption command. It
* also contains statistics about how far this single batch has been executed.
|
* Class that encapsulates re-encryption details of a file.",8-375,"Class for finalizing re-encrypt EDEK operations. Class representing the task for one batch of a re-encryption command. It
* also contains statistics about how far this single batch has been executed. Class that encapsulates re-encryption details of a file.","by updating file xattrs with * edeks returned from reencryption. * <p> * The tasks are submitted by ReencryptionHandler. * <p>. It is assumed only 1 Updater will be running, since updating file xattrs
* requires namespace write lock, and performance gain from multi-threading
* is limited.
|
* Class to track re-encryption submissions of a single zone. It contains
* all the submitted futures, and statistics about how far the futures are
* processed. It contains the
* file inode, stores the initial edek of the file, and the new edek
* after re-encryption.
* <p>
* Assumptions are the object initialization happens when dir lock is held,
* and inode is valid and is encrypted during initialization.
* <p>
* Namespace changes may happen during re-encryption, and if inode is changed
* the re-encryption is skipped.",Class for finalizing re-encrypt EDEK operations,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,BalancingPolicy.java,"* Balancing policy.
* Since a datanode may contain multiple block pools,
* {@link Pool} implies {@link Node}
* but NOT the other way around
|
* Cluster is balanced if each node is balanced.
|
* Cluster is balanced if each pool in each node is balanced.",8-375,* Balancing policy.,"Since a datanode may contain multiple block pools, * {@link Pool} implies {@link Node} * but NOT the other way around |",Cluster is balanced if each node is balanced. | * Cluster is balanced if each pool in each node is balanced.,,,,,,,,,,,"Since a datanode may contain multiple block pools, * {@link Pool} implies {@link Node} * but NOT the other way around |",,,,,,,,,
E2,E1 (agree),,,GetNodesToAttributesResponse.java,"* <p>
* The response sent by the <code>ResourceManager</code> to a client requesting
* nodes to attributes mapping.
* </p>
*
* @see ApplicationClientProtocol#getNodesToAttributes
* (GetNodesToAttributesRequest)",8-375,The response sent by the <code>ResourceManager</code> to a client requesting * nodes to attributes mapping.,,,,,,,,,,,,,@see ApplicationClientProtocol#getNodesToAttributes * (GetNodesToAttributesRequest),,,,,,,,,
E2,E1 (agree),,,SwiftRestClient.java,"* This implements the client-side of the Swift REST API.
*
* The core actions put, get and query data in the Swift object store,
* after authenticating the client.
*
* <b>Logging:</b>
*
* Logging at DEBUG level displays detail about the actions of this
* client, including HTTP requests and responses -excluding authentication
* details.
|
* Get the current operation statistics.
* @return a snapshot of the statistics
|
* Base class for all Swift REST operations.
*
* @param <M> request
* @param <R> result
|
* Get the current operation statistics.
* @return a snapshot of the statistics
|
* There's a special type for auth messages, so that low-level
* message handlers can react to auth failures differently from everything
* else.
|
* Get the current operation statistics.
* @return a snapshot of the statistics
|
* Generate an auth message.
* @param <R> response
|
* Get the current operation statistics.
* @return a snapshot of the statistics
|
* Create operation.
*
* @param <R> result type
|
* Get the current operation statistics.
* @return a snapshot of the statistics
|
* Delete operation.
*
* @param <R>",8-375,* This implements the client-side of the Swift REST API.,"The core actions put, get and query data in the Swift object store, * after authenticating the client. * * <b>Logging:</b> * * Logging at DEBUG level displays detail about the actions of this * client, including HTTP requests and responses -excluding authentication * details.",,,,"* There's a special type for auth messages, so that low-level
* message handlers can react to auth failures differently from everything
* else.
|",,,,,,,,"Get the current operation statistics. * @return a snapshot of the statistics. Base class for all Swift REST operations.
*
* @param <M> request
* @param <R> result
|
* Get the current operation statistics.
* @return a snapshot of the statistics
|
* Get the current operation statistics.
* @return a snapshot of the statistics
|
* Generate an auth message.
* @param <R> response
|
* Get the current operation statistics.
* @return a snapshot of the statistics
|
* Create operation.
*
* @param <R> result type
|
* Get the current operation statistics.
* @return a snapshot of the statistics
|
* Delete operation.
*
* @param <R>",,,,,,,,,
E2,E1 (agree),,,Classpath.java,"* Command-line utility for getting the full classpath needed to launch a Hadoop
* client application. If the hadoop script is called with ""classpath"" as the
* command, then it simply prints the classpath and exits immediately without
* launching a JVM. The output likely will include wildcards in the classpath.
* If there are arguments passed to the classpath command, then this class gets
* called. With the --glob argument, it prints the full classpath with wildcards
* expanded. This is useful in situations where wildcard syntax isn't usable.
* With the --jar argument, it writes the classpath as a manifest in a jar file.
* This is useful in environments with short limitations on the maximum command
* line length, where it may not be possible to specify the full classpath in a
* command. For example, the maximum command line length on Windows is 8191
* characters.",8-375,Command-line utility for getting the full classpath needed to launch a Hadoop * client application.,"If the hadoop script is called with ""classpath"" as the * command, then it simply prints the classpath and exits immediately without * launching a JVM. The output likely will include wildcards in the classpath. * If there are arguments passed to the classpath command, then this class gets * called. With the --glob argument, it prints the full classpath with wildcards * expanded. This is useful in situations where wildcard syntax isn't usable. * With the --jar argument, it writes the classpath as a manifest in a jar file. * This is useful in environments with short limitations on the maximum command * line length, where it may not be possible to specify the full classpath in a * command. For example, the maximum command line length on Windows is 8191 * characters.",,,,,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,StructureGenerator.java,"* This program generates a random namespace structure with the following
* constraints:
* 1. The number of subdirectories is a random number in [minWidth, maxWidth].
* 2. The maximum depth of each subdirectory is a random number * [2*maxDepth/3, maxDepth].
* 3. Files are randomly placed in the empty directories. The size of each
* file follows Gaussian distribution.
* The generated namespace structure is described by two files in the output
* directory. Each line of the first file * contains the full name of a leaf directory. * Each line of the second file contains
* the full name of a file and its size, separated by a blank.
* * The synopsis of the command is
* java StructureGenerator
-maxDepth <maxDepth> : maximum depth of the directory tree; default is 5.
-minWidth <minWidth> : minimum number of subdirectories per directories; default is 1
-maxWidth <maxWidth> : maximum number of subdirectories per directories; default is 5
-numOfFiles <#OfFiles> : the total number of files; default is 10.
-avgFileSize <avgFileSizeInBlocks>: average size of blocks; default is 1.
-outDir <outDir>: output directory; default is the current directory.
-seed <seed>: random number generator seed; default is the current time.
| In memory representation of a directory | In memory representation of a file",8-375,This program generates a random namespace structure,"This program generates a random namespace structure with the following
* constraints:
* 1. The number of subdirectories is a random number in [minWidth, maxWidth].
* 2. The maximum depth of each subdirectory is a random number * [2*maxDepth/3, maxDepth].
* 3. Files are randomly placed in the empty directories. The size of each
* file follows Gaussian distribution.
* The generated namespace structure is described by two files in the output
* directory. Each line of the first file * contains the full name of a leaf directory. * Each line of the second file contains
* the full name of a file and its size, separated by a blank.",,,"* The synopsis of the command is
* java StructureGenerator
-maxDepth <maxDepth> : maximum depth of the directory tree; default is 5.
-minWidth <minWidth> : minimum number of subdirectories per directories; default is 1
-maxWidth <maxWidth> : maximum number of subdirectories per directories; default is 5
-numOfFiles <#OfFiles> : the total number of files; default is 10.
-avgFileSize <avgFileSizeInBlocks>: average size of blocks; default is 1.
-outDir <outDir>: output directory; default is the current directory.
-seed <seed>: random number generator seed; default is the current time.
| In memory representation of a directory | In memory representation of a file",,,,,,,,,,,,,,,,,,
E2,E1 (agree),,,FileSystemApplicationHistoryStore.java,"* File system implementation of {@link ApplicationHistoryStore}. In this
* implementation, one application will have just one file in the file system,
* which contains all the history data of one application, and its attempts and
* containers. {@link #applicationStarted(ApplicationStartData)} is supposed to
* be invoked first when writing any history data of one application and it will
* open a file, while {@link #applicationFinished(ApplicationFinishData)} is
* supposed to be last writing operation and will close the file.",8-375,* File system implementation of {@link ApplicationHistoryStore}.,"In this * implementation, one application will have just one file in the file system, * which contains all the history data of one application, and its attempts and * containers. {@link #applicationStarted(ApplicationStartData)} is supposed to
* be invoked first when writing any history data of one application and it will
* open a file, while {@link #applicationFinished(ApplicationFinishData)} is
* supposed to be last writing operation and will close the file.",,,,,,,,,,,,"* File system implementation of {@link ApplicationHistoryStore}. {@link #applicationStarted(ApplicationStartData)} is supposed to
* be invoked first when writing any history data of one application and it will
* open a file, while {@link #applicationFinished(ApplicationFinishData)} is
* supposed to be last writing operation and will close the file.",,,,,,,,,
E2,E1 (disagree),,,AbstractFSContract.java,"* Class representing a filesystem contract that a filesystem
* implementation is expected implement.
*
* Part of this contract class is to allow FS implementations to
* provide specific opt outs and limits, so that tests can be
* skip unsupported features (e.g. case sensitivity tests),
* dangerous operations (e.g. trying to delete the root directory),
* and limit filesize and other numeric variables for scale tests",8-375,Class representing a filesystem contract that a filesystem * implementation is expected implement.,,"Part of this contract class is to allow FS implementations to * provide specific opt outs and limits, so that tests can be * skip unsupported features (e.g. case sensitivity tests), * dangerous operations (e.g. trying to delete the root directory), * and limit filesize and other numeric variables for scale tests",,,,,,,,,,,,,,,,,,,,
E2,,E3 (disagree),,ApplicationConstants.java,"* This is the API for the applications comprising of constants that YARN sets
* up for the applications and the containers.
*
* TODO: Investigate the semantics and security of each cross-boundary refs.
|
* The type of launch for the container.
|
* Environment for Applications.
*
* Some of the environment variables for applications are <em>final</em>
* i.e. they cannot be modified by the applications.",8-375,* This is the API for the applications comprising of constants that YARN sets * up for the applications and the containers.,The type of launch for the container. | * Environment for Applications. * * Some of the environment variables for applications are <em>final</em> * i.e. they cannot be modified by the applications.,,,,,TODO: Investigate the semantics and security of each cross-boundary refs.,,,,,,,,,,"* Some of the environment variables for applications are <em>final</em>
* i.e. they cannot be modified by the applications.",,,,,,
E2,,E3 (agree),,DiskBalancerCluster.java,"* DiskBalancerCluster represents the nodes that we are working against.
* <p>
* Please Note :
* Semantics of inclusionList and exclusionLists.
* <p>
* If a non-empty inclusionList is specified then the diskBalancer assumes that
* the user is only interested in processing that list of nodes. This node list
* is checked against the exclusionList and only the nodes in inclusionList but
* not in exclusionList is processed.
* <p>
* if inclusionList is empty, then we assume that all live nodes in the nodes is
* to be processed by diskBalancer. In that case diskBalancer will avoid any
* nodes specified in the exclusionList but will process all nodes in the
* cluster.
* <p>
* In other words, an empty inclusionList is means all the nodes otherwise
* only a given list is processed and ExclusionList is always honored.",8-375,DiskBalancerCluster represents the nodes that we are working against.,"Please Note :
* Semantics of inclusionList and exclusionLists.
* <p>
* If a non-empty inclusionList is specified then the diskBalancer assumes that
* the user is only interested in processing that list of nodes. This node list
* is checked against the exclusionList and only the nodes in inclusionList but
* not in exclusionList is processed.
* <p>
* if inclusionList is empty, then we assume that all live nodes in the nodes is
* to be processed by diskBalancer. In that case diskBalancer will avoid any
* nodes specified in the exclusionList but will process all nodes in the
* cluster.
* <p>
* In other words, an empty inclusionList is means all the nodes otherwise
* only a given list is processed and ExclusionList is always honored.",,,,,,,,,,,,,,,,,,,,,
E2,,E3 (disagree),,QuasiMonteCarlo.java,"* A map/reduce program that estimates the value of Pi
* using a quasi-Monte Carlo (qMC) method.
* Arbitrary integrals can be approximated numerically by qMC methods.
* In this example,
* we use a qMC method to approximate the integral $I = \int_S f(x) dx$,
* where $S=[0,1)^2$ is a unit square,
* $x=(x_1,x_2)$ is a 2-dimensional point,
* and $f$ is a function describing the inscribed circle of the square $S$,
* $f(x)=1$ if $(2x_1-1)^2+(2x_2-1)^2 &lt;= 1$ and $f(x)=0$, otherwise.
* It is easy to see that Pi is equal to $4I$.
* So an approximation of Pi is obtained once $I$ is evaluated numerically.
* * There are better methods for computing Pi.
* We emphasize numerical approximation of arbitrary integrals in this example.
* For computing many digits of Pi, consider using bbp.
*
* The implementation is discussed below.
*
* Mapper:
* Generate points in a unit square
* and then count points inside/outside of the inscribed circle of the square.
*
* Reducer:
* Accumulate points inside/outside results from the mappers.
*
* Let numTotal = numInside + numOutside.
* The fraction numInside/numTotal is a rational approximation of
* the value (Area of the circle)/(Area of the square) = $I$,
* where the area of the inscribed circle is Pi/4
* and the area of unit square is 1.
* Finally, the estimated value of Pi is 4(numInside/numTotal). | 2-dimensional Halton sequence {H(i)},
* where H(i) is a 2-dimensional point and i >= 1 is the index.
* Halton sequence is used to generate sample points for Pi estimation. |
* Mapper class for Pi estimation.
* Generate points in a unit square
* and then count points inside/outside of the inscribed circle of the square.
|
* Reducer class for Pi estimation.
* Accumulate points inside/outside results from the mappers.",8-375,* A map/reduce program that estimates the value of Pi.,"using a quasi-Monte Carlo (qMC) method. Arbitrary integrals can be approximated numerically by qMC methods. * There are better methods for computing Pi.
* We emphasize numerical approximation of arbitrary integrals in this example.
* For computing many digits of Pi, consider using bbp.
*
* The implementation is discussed below.
*
* Mapper:
* Generate points in a unit square
* and then count points inside/outside of the inscribed circle of the square.
*
* Reducer:
* Accumulate points inside/outside results from the mappers. * Mapper class for Pi estimation.
* Generate points in a unit square
* and then count points inside/outside of the inscribed circle of the square.
|
* Reducer class for Pi estimation.
* Accumulate points inside/outside results from the mappers.",,," In this example,
* we use a qMC method to approximate the integral $I = \int_S f(x) dx$,
* where $S=[0,1)^2$ is a unit square,
* $x=(x_1,x_2)$ is a 2-dimensional point,
* and $f$ is a function describing the inscribed circle of the square $S$,
* $f(x)=1$ if $(2x_1-1)^2+(2x_2-1)^2 &lt;= 1$ and $f(x)=0$, otherwise.
* It is easy to see that Pi is equal to $4I$.
* So an approximation of Pi is obtained once $I$ is evaluated numerically.
* * There are better methods for computing Pi.
* We emphasize numerical approximation of arbitrary integrals in this example.
* For computing many digits of Pi, consider using bbp.
*",,,,,,,,,"In this example,
* we use a qMC method to approximate the integral $I = \int_S f(x) dx$,
",,,,"For computing many digits of Pi, consider using bbp.",,,,,
E2,,E3 (disagree),,TimelineDataToRetrieve.java,"* Encapsulates information regarding which data to retrieve for each entity
* while querying.<br>
* Data to retrieve contains the following :<br>
* <ul>
* <li><b>confsToRetrieve</b> - Used for deciding which configs to return
* in response. This is represented as a {@link TimelineFilterList} object
* containing {@link TimelinePrefixFilter} objects. These can either be
* exact config keys' or prefixes which are then compared against config
* keys' to decide configs(inside entities) to return in response. If null
* or empty, all configurations will be fetched if fieldsToRetrieve
* contains {@link Field#CONFIGS} or {@link Field#ALL}. This should not be
* confused with configFilters which is used to decide which entities to
* return instead.</li>
* <li><b>metricsToRetrieve</b> - Used for deciding which metrics to return
* in response. This is represented as a {@link TimelineFilterList} object
* containing {@link TimelinePrefixFilter} objects. These can either be
* exact metric ids' or prefixes which are then compared against metric
* ids' to decide metrics(inside entities) to return in response. If null
* or empty, all metrics will be fetched if fieldsToRetrieve contains
* {@link Field#METRICS} or {@link Field#ALL}. This should not be confused
* with metricFilters which is used to decide which entities to return
* instead.</li>
* <li><b>fieldsToRetrieve</b> - Specifies which fields of the entity
* object to retrieve, see {@link Field}. If null, retrieves 3 fields,
* namely entity id, entity type and entity created time. All fields will
* be returned if {@link Field#ALL} is specified.</li>
* <li><b>metricsLimit</b> - If fieldsToRetrieve contains METRICS/ALL or
* metricsToRetrieve is specified, this limit defines an upper limit to the
* number of metrics to return. This parameter is ignored if METRICS are not to
* be fetched.</li>
* <li><b>metricsTimeStart</b> - Metric values before this timestamp would not
* be retrieved. If null or {@literal <0}, defaults to 0.</li>
* <li><b>metricsTimeEnd</b> - Metric values after this timestamp would not
* be retrieved. If null or {@literal <0}, defaults to {@link Long#MAX_VALUE}.
* </li>
* </ul>",8-375,"Encapsulates information regarding which data to retrieve for each entity
* while querying.<br>","* Data to retrieve contains the following :<br>
* <ul>
* <li><b>confsToRetrieve</b> - Used for deciding which configs to return
* in response. This is represented as a {@link TimelineFilterList} object
* containing {@link TimelinePrefixFilter} objects. These can either be
* exact config keys' or prefixes which are then compared against config
* keys' to decide configs(inside entities) to return in response. If null
* or empty, all configurations will be fetched if fieldsToRetrieve
* contains {@link Field#CONFIGS} or {@link Field#ALL}. This should not be
* confused with configFilters which is used to decide which entities to
* return instead.</li>
* <li><b>metricsToRetrieve</b> - Used for deciding which metrics to return
* in response. This is represented as a {@link TimelineFilterList} object
* containing {@link TimelinePrefixFilter} objects. These can either be
* exact metric ids' or prefixes which are then compared against metric
* ids' to decide metrics(inside entities) to return in response. If null
* or empty, all metrics will be fetched if fieldsToRetrieve contains
* {@link Field#METRICS} or {@link Field#ALL}. This should not be confused
* with metricFilters which is used to decide which entities to return
* instead.</li>
* <li><b>fieldsToRetrieve</b> - Specifies which fields of the entity
* object to retrieve, see {@link Field}. If null, retrieves 3 fields,
* namely entity id, entity type and entity created time. All fields will
* be returned if {@link Field#ALL} is specified.</li>
* <li><b>metricsLimit</b> - If fieldsToRetrieve contains METRICS/ALL or
* metricsToRetrieve is specified, this limit defines an upper limit to the
* number of metrics to return. This parameter is ignored if METRICS are not to
* be fetched.</li>
* <li><b>metricsTimeStart</b> - Metric values before this timestamp would not
* be retrieved. If null or {@literal <0}, defaults to 0.</li>
* <li><b>metricsTimeEnd</b> - Metric values after this timestamp would not
* be retrieved. If null or {@literal <0}, defaults to {@link Long#MAX_VALUE}.
* </li>
* </ul>",,,,,,,,,,,,"* Data to retrieve contains the following :<br>
* <ul>
* <li><b>confsToRetrieve</b> - Used for deciding which configs to return
* in response. This is represented as a {@link TimelineFilterList} object
* containing {@link TimelinePrefixFilter} objects. These can either be
* exact config keys' or prefixes which are then compared against config
* keys' to decide configs(inside entities) to return in response. If null
* or empty, all configurations will be fetched if fieldsToRetrieve
* contains {@link Field#CONFIGS} or {@link Field#ALL}. This should not be
* confused with configFilters which is used to decide which entities to
* return instead.</li>
* <li><b>metricsToRetrieve</b> - Used for deciding which metrics to return
* in response. This is represented as a {@link TimelineFilterList} object
* containing {@link TimelinePrefixFilter} objects. These can either be
* exact metric ids' or prefixes which are then compared against metric
* ids' to decide metrics(inside entities) to return in response. If null
* or empty, all metrics will be fetched if fieldsToRetrieve contains
* {@link Field#METRICS} or {@link Field#ALL}. This should not be confused
* with metricFilters which is used to decide which entities to return
* instead.</li>
* <li><b>fieldsToRetrieve</b> - Specifies which fields of the entity
* object to retrieve, see {@link Field}. If null, retrieves 3 fields,
* namely entity id, entity type and entity created time. All fields will
* be returned if {@link Field#ALL} is specified.</li>
* <li><b>metricsLimit</b> - If fieldsToRetrieve contains METRICS/ALL or
* metricsToRetrieve is specified, this limit defines an upper limit to the
* number of metrics to return. This parameter is ignored if METRICS are not to
* be fetched.</li>
* <li><b>metricsTimeStart</b> - Metric values before this timestamp would not
* be retrieved. If null or {@literal <0}, defaults to 0.</li>
* <li><b>metricsTimeEnd</b> - Metric values after this timestamp would not
* be retrieved. If null or {@literal <0}, defaults to {@link Long#MAX_VALUE}.
* </li>
* </ul>",,," <li>This should not be
* confused with configFilters which is used to decide which entities to
* return instead.</li>
<li>This should not be confused
* with metricFilters which is used to decide which entities to return
* instead.</li>
This parameter is ignored if METRICS are not to
* be fetched.</li>
* <li><b>metricsTimeStart</b> - Metric values before this timestamp would not
* be retrieved. If null or {@literal <0}, defaults to 0.</li>
* <li><b>metricsTimeEnd</b> - Metric values after this timestamp would not
* be retrieved. If null or {@literal <0}, defaults to {@link Long#MAX_VALUE}.
* </li>
* </ul>",,,,,,
E2,,E3 (agree),,EventWatcher.java,"* Event watcher the (re)send a message after timeout.
* <p>
* Event watcher will send the tracked payload/event after a timeout period
* unless a confirmation from the original event (completion event) is arrived.
*
* @param <TIMEOUT_PAYLOAD> The type of the events which are tracked.
* @param <COMPLETION_PAYLOAD> The type of event which could cancel the
* tracking.",8-375,* Event watcher the (re)send a message after timeout.,,"Event watcher will send the tracked payload/event after a timeout period
* unless a confirmation from the original event (completion event) is arrived.",,,,,,,,,,,"* @param <TIMEOUT_PAYLOAD> The type of the events which are tracked.
* @param <COMPLETION_PAYLOAD> The type of event which could cancel the
* tracking.",,,,,,,,,
E2,,E3 (disagree),,ComparableVersion.java,"Code source of this file:| http://grepcode.com/file/repo1.maven.org/maven2/| org.apache.maven/maven-artifact/3.1.1/| org/apache/maven/artifact/versioning/ComparableVersion.java/|| Modifications made on top of the source:| 1. Changed| package org.apache.maven.artifact.versioning;| to| package org.apache.hadoop.util;| 2. Removed author tags to clear hadoop author tag warning|
* Generic implementation of version comparison.
* * <p>Features:
* <ul>
* <li>mixing of '<code>-</code>' (dash) and '<code>.</code>' (dot) separators,</li>
* <li>transition between characters and digits also constitutes a separator:
* <code>1.0alpha1 =&gt; [1, 0, alpha, 1]</code></li>
* <li>unlimited number of version components,</li>
* <li>version components in the text can be digits or strings,</li>
* <li>strings are checked for well-known qualifiers and the qualifier ordering is used for version ordering.
* Well-known qualifiers (case insensitive) are:<ul>
* <li><code>alpha</code> or <code>a</code></li>
* <li><code>beta</code> or <code>b</code></li>
* <li><code>milestone</code> or <code>m</code></li>
* <li><code>rc</code> or <code>cr</code></li>
* <li><code>snapshot</code></li>
* <li><code>(the empty string)</code> or <code>ga</code> or <code>final</code></li>
* <li><code>sp</code></li>
* </ul>
* Unknown qualifiers are considered after known qualifiers, with lexical order (always case insensitive),
* </li>
* <li>a dash usually precedes a qualifier, and is always less important than something preceded with a dot.</li>
* </ul><p>
*
* @see <a href=""https://cwiki.apache.org/confluence/display/MAVENOLD/Versioning"">""Versioning"" on Maven Wiki</a>
|
* Represents a numeric item in the version item list.
|
* Represents a string in the version item list, usually a qualifier.
|
* Represents a version list item. This class is used both for the global item list and for sub-lists (which start
* with '-(number)' in the version specification).",8-375,"Generic implementation of version comparison. Represents a numeric item in the version item list.
|
* Represents a string in the version item list, usually a qualifier.
|
* Represents a version list item. This class is used both for the global item list and for sub-lists (which start
* with '-(number)' in the version specification).",,,,"* Generic implementation of version comparison.
* * <p>Features:
* <ul>
* <li>mixing of '<code>-</code>' (dash) and '<code>.</code>' (dot) separators,</li>
* <li>transition between characters and digits also constitutes a separator:
* <code>1.0alpha1 =&gt; [1, 0, alpha, 1]</code></li>
* <li>unlimited number of version components,</li>
* <li>version components in the text can be digits or strings,</li>
* <li>strings are checked for well-known qualifiers and the qualifier ordering is used for version ordering.
* Well-known qualifiers (case insensitive) are:<ul>
* <li><code>alpha</code> or <code>a</code></li>
* <li><code>beta</code> or <code>b</code></li>
* <li><code>milestone</code> or <code>m</code></li>
* <li><code>rc</code> or <code>cr</code></li>
* <li><code>snapshot</code></li>
* <li><code>(the empty string)</code> or <code>ga</code> or <code>final</code></li>
* <li><code>sp</code></li>
* </ul>
* Unknown qualifiers are considered after known qualifiers, with lexical order (always case insensitive),
* </li>
* <li>a dash usually precedes a qualifier, and is always less important than something preceded with a dot.</li>
* </ul><p>",,,,,,,,,"Code source of this file:| http://grepcode.com/file/repo1.maven.org/maven2/| org.apache.maven/maven-artifact/3.1.1/| org/apache/maven/artifact/versioning/ComparableVersion.java/|| Modifications made on top of the source:| 1. Changed| package org.apache.maven.artifact.versioning;| to| package org.apache.hadoop.util;| 2. Removed author tags to clear hadoop author tag warning|. * @see <a href=""https://cwiki.apache.org/confluence/display/MAVENOLD/Versioning"">""Versioning"" on Maven Wiki</a>",,,,,,,,,