Evaluator,Reviewer 1,Reviewer 2,Project,Class,Comment,comment length,summary,Expand,rational,deprecation,usage,exception,todo,Incomplete,Commented code,directive,formatter,License,Ownership,Pointer,Auto generated,Noise,Warning,Recommendation,Precondition,Coding Guidelines,Extension,Subclass explnation,Observation
E3,,E4 (agree),Spark,RemoveBlocks.java,Request to remove a set of blocks.,1,Request to remove a set of blocks.,,,,,,,,,,,,,,,,,,,,,,
E3,,E4 (agree),,JavaRecord.java, Java Bean class to be used with the example JavaSqlNetworkWordCount. ,1,,,,, Java Bean class to be used with the example JavaSqlNetworkWordCount. ,,,,,,,,,,,,,,,,,,
E3,,E4 (agree),,TBoolColumn.java," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ",1," The set of fields this struct contains, along with convenience methods for finding and manipulating them. ",,,,,,,,,,,,,,,,,,,,,,
E3,,E4 (agree),,FlatMapFunction.java,"
 * A function that returns zero or more output records from each input record.",2,"
 * A function that returns zero or more output records from each input record.",,,,,,,,,,,,,,,,,,,,,,
E3,E1,,,LevelDB.java," * Implementation of KVStore that uses LevelDB as the underlying data store.
 | Needs to be public for Jackson. ",3, * Implementation of KVStore that uses LevelDB as the underlying data store.,,,,,,,,,,,,,,,,,, | Needs to be public for Jackson. ,,,,
E3,E1,,,LocalDiskShuffleDataIO.java,"
 * Implementation of the {@link ShuffleDataIO} plugin system that replicates the local shuffle
 * storage and index file functionality that has historically been used from Spark 2.4 and earlier.",4,"
 * Implementation of the {@link ShuffleDataIO} plugin system that replicates the local shuffle
 * storage and index file functionality that has historically been used from Spark 2.4 and earlier.",,,,,,,,,,,,,{@link ShuffleDataIO} ,,, that has historically been used from Spark 2.4 and earlier.,,,,,,
E3,E1,,,StreamHandle.java,"
 * Identifier for a fixed number of chunks to read from a stream created by an ""open blocks""
 * message. This is used by {@link org.apache.spark.network.shuffle.OneForOneBlockFetcher}.",4," Identifier for a fixed number of chunks to read from a stream created by an ""open blocks""
 * message",,,,,,,,,,,,, This is used by {@link org.apache.spark.network.shuffle.OneForOneBlockFetcher}.,,,,,,,,,
E3,E1,,,MessageWithHeader.java,"
 * A wrapper message that holds two separate pieces (a header and a body).
 *
 * The header must be a ByteBuf, while the body can be a ByteBuf or a FileRegion.
 ",5, * A wrapper message that holds two separate pieces (a header and a body).,,,,,,,,,,,,,,,,,,"The header must be a ByteBuf, while the body can be a ByteBuf or a FileRegion.",,,,
E3,E2,,,UploadBlockStream.java,"
 * A request to Upload a block, which the destination should receive as a stream.
 *
 * The actual block data is not contained here.  It will be passed to the StreamCallbackWithID
 * that is returned from RpcHandler.receiveStream()",6," * A request to Upload a block, which the destination should receive as a stream.","  * The actual block data is not contained here.  It will be passed to the StreamCallbackWithID
 * that is returned from RpcHandler.receiveStream()",,,,,,,,,,,,,,,  * The actual block data is not contained here.,,,,,,
E3,E2,,,ChunkFetchSuccess.java,"
 * Response to {@link ChunkFetchRequest} when a chunk exists and has been successfully fetched.
 *
 * Note that the server-side encoding of this messages does NOT include the buffer itself, as this
 * may be written by Netty in a more efficient manner (i.e., zero-copy write).
 * Similarly, the client-side decoding will reuse the Netty ByteBuf as the buffer.",7,"
 * Response to {@link ChunkFetchRequest} when a chunk exists and has been successfully fetched."," * Similarly, the client-side decoding will reuse the Netty ByteBuf as the buffer.",,,,,,,,,,,,,,," * Note that the server-side encoding of this messages does NOT include the buffer itself, as this
 * may be written by Netty in a more efficient manner (i.e., zero-copy write).
 * Similarly, the client-side decoding will reuse the Netty ByteBuf as the buffer.",,,,,,
E3,E2,,,JavaStreamingTestExample.java,"
 * Perform streaming testing using Welch's 2-sample t-test on a stream of data, where the data
 * stream arrives as text files in a directory. Stops when the two groups are statistically
 * significant (p-value < 0.05) or after a user-specified timeout in number of batches is exceeded.
 *
 * The rows of the text files must be in the form `Boolean, Double`. For example:
 *   false, -3.92
 *   true, 99.32
 *
 * Usage:
 *   JavaStreamingTestExample <dataDir> <batchDuration> <numBatchesTimeout>
 *
 * To run on your local machine using the directory `dataDir` with 5 seconds between each batch and
 * a timeout after 100 insignificant batches, call:
 *    $ bin/run-example mllib.JavaStreamingTestExample dataDir 5 100
 *
 * As you add text files to `dataDir` the significance test wil continually update every
 * `batchDuration` seconds until the test becomes significant (p-value < 0.05) or the number of
 * batches processed exceeds `numBatchesTimeout`.
 ",20," * Perform streaming testing using Welch's 2-sample t-test on a stream of data, where the data
 * stream arrives as text files in a directory. Stops when the two groups are statistically
 * significant (p-value < 0.05) or after a user-specified timeout in number of batches is exceeded."," The rows of the text files must be in the form `Boolean, Double`. For example:
 *   false, -3.92
 *   true, 99.32",,," Usage:
 *   JavaStreamingTestExample <dataDir> <batchDuration> <numBatchesTimeout>
 *
 * To run on your local machine using the directory `dataDir` with 5 seconds between each batch and
 * a timeout after 100 insignificant batches, call:
 *    $ bin/run-example mllib.JavaStreamingTestExample dataDir 5 100
 *
 * As you add text files to `dataDir` the significance test wil continually update every
 * `batchDuration` seconds until the test becomes significant (p-value < 0.05) or the number of
 * batches processed exceeds `numBatchesTimeout`.
 ",,,,,,,,,,,,,,,,,,
E3,E2,,,KVStore.java,"
 * Abstraction for a local key/value store for storing app data.
 *
 * <p>
 * There are two main features provided by the implementations of this interface:
 * </p>
 *
 * <h3>Serialization</h3>
 *
 * <p>
 * If the underlying data store requires serialization, data will be serialized to and deserialized
 * using a {@link KVStoreSerializer}, which can be customized by the application. The serializer is
 * based on Jackson, so it supports all the Jackson annotations for controlling the serialization of
 * app-defined types.
 * </p>
 *
 * <p>
 * Data is also automatically compressed to save disk space.
 * </p>
 *
 * <h3>Automatic Key Management</h3>
 *
 * <p>
 * When using the built-in key management, the implementation will automatically create unique
 * keys for each type written to the store. Keys are based on the type name, and always start
 * with the ""+"" prefix character (so that it's easy to use both manual and automatic key
 * management APIs without conflicts).
 * </p>
 *
 * <p>
 * Another feature of automatic key management is indexing; by annotating fields or methods of
 * objects written to the store with {@link KVIndex}, indices are created to sort the data
 * by the values of those properties. This makes it possible to provide sorting without having
 * to load all instances of those types from the store.
 * </p>
 *
 * <p>
 * KVStore instances are thread-safe for both reads and writes.
 * </p>
 ",40,"
 * Abstraction for a local key/value store for storing app data."," <p>
 * There are two main features provided by the implementations of this interface:
 * </p>
 *
 * <h3>Serialization</h3>
 *
 * <p>
 * If the underlying data store requires serialization, data will be serialized to and deserialized
 * using a {@link KVStoreSerializer}, which can be customized by the application. The serializer is
 * based on Jackson, so it supports all the Jackson annotations for controlling the serialization of
 * app-defined types.
 * </p>
 *
 * <p>
 * Data is also automatically compressed to save disk space.
 * </p>
 *
 * <h3>Automatic Key Management</h3>
 *
 * <p>
 * When using the built-in key management, the implementation will automatically create unique
 * keys for each type written to the store. Keys are based on the type name, and always start
 * with the ""+"" prefix character (so that it's easy to use both manual and automatic key
 * management APIs without conflicts).
 * </p>
 *
 * <p>
 * Another feature of automatic key management is indexing; by annotating fields or methods of
 * objects written to the store with {@link KVIndex}, indices are created to sort the data
 * by the values of those properties. This makes it possible to provide sorting without having
 * to load all instances of those types from the store.
 * </p>
 *
 * <p>
 * KVStore instances are thread-safe for both reads and writes.
 * </p>
 ",,,,,,,,,,,,"@link KVStoreSerializer
@link KVIndex
",,,,,,,,,KVStore instances are thread-safe for both reads and writes.
E4,,E3 (agree),,ExecutorPlugin.java,"* A plugin which can be automatically instantiated within each Spark executor. Users can specify
* plugins which should be created with the ""spark.executor.plugins"" configuration. An instance
* of each plugin will be created for every executor, including those created by dynamic allocation,
* before the executor starts running any tasks.
*
* The specific api exposed to the end users still considered to be very unstable. We will
* hopefully be able to keep compatibility by providing default implementations for any methods
* added, but make no guarantees this will always be possible across all Spark releases.
*
* Spark does nothing to verify the plugin is doing legitimate things, or to manage the resources
* it uses. A plugin acquires the same privileges as the user running the task. A bad plugin
* could also interfere with task execution and make the executor fail in unexpected ways.",14,A plugin which can be automatically instantiated within each Spark executor.,"Users can specify plugins which should be created with the ""spark.executor.plugins"" configuration. An instance of each plugin will be created for every executor, including those created by dynamic allocation, before the executor starts running any tasks.",,,,,,,,,,,,,,,"* The specific api exposed to the end users still considered to be very unstable. We will
* hopefully be able to keep compatibility by providing default implementations for any methods
* added, but make no guarantees this will always be possible across all Spark releases.
* Spark does nothing to verify the plugin is doing legitimate things, or to manage the resources
* it uses. A plugin acquires the same privileges as the user running the task. A bad plugin
* could also interfere with task execution and make the executor fail in unexpected ways.",,,,,,