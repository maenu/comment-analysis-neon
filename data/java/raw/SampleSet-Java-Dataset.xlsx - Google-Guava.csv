Evaluator,Reviewer 1,Reviewer 2,Project,Class,Comment,comment length,summary,Exapnd,rational,deprecation,usage,exception,todo,Incomplete,Commented code,directive,formatter,License,Ownership,Pointer,Auto generated,Noise,Warnings,Recommendation,Precondition,Coding Guidelines,Extension,Subclass explnation,Observation
E3,,E4 (agree),,ImmutableSetHashFloodingDetectionBenchmark.java, Benchmark of implementations of {@link ImmutableSet#hashFloodingDetected(Object[])}. ,1, Benchmark of implementations of {@link ImmutableSet#hashFloodingDetected(Object[])}. ,,,,,,,,,,,,, {@link ImmutableSet#hashFloodingDetected(Object[])}. ,,,,,,,,,
E3,,E4 (agree),,ForwardingImmutableSet.java,"
 * GWT implementation of {@link ImmutableSet} that forwards to another {@code Set} implementation.
 *
 * @author Hayward Chan",5, GWT implementation of {@link ImmutableSet} that forwards to another {@code Set} implementation.,,,,,,,,,,,, * @author Hayward Chan, {@link ImmutableSet} ,,,,,,,,,
E3,,E4 (agree),,RegularImmutableList_CustomFieldSerializer.java,"
 * This class implements the GWT serialization of {@link RegularImmutableList}.
 *
 * @author Hayward Chan",5,"
 * This class implements the GWT serialization of {@link RegularImmutableList}.",,,,,,,,,,,, * @author Hayward Chan, {@link RegularImmutableList}.,,,,,,,,,
E3,E1,,,DescendingMultiset.java,"
 * A skeleton implementation of a descending multiset. Only needs {@code forwardMultiset()} and
 * {@code entryIterator()}.
 *
 * @author Louis Wasserman",6,"
 * A skeleton implementation of a descending multiset. ",,,,,,,,,,,, * @author Louis Wasserman,"Only needs {@code forwardMultiset()} and
 * {@code entryIterator()}.",,,,,"Only needs {@code forwardMultiset()} and
 * {@code entryIterator()}.",,,,
E3,E1,,,MapPutAllTester.java,"
 * A generic JUnit test which tests {@code putAll} operations on a map. Can't be invoked directly;
 * please see {@link com.google.common.collect.testing.MapTestSuiteBuilder}.
 *
 * @author Chris Povirk
 * @author Kevin Bourrillion",7, A generic JUnit test which tests {@code putAll} operations on a map,,,,,,,,,,,," * @author Chris Povirk
 * @author Kevin Bourrillion","
 * please see {@link com.google.common.collect.testing.MapTestSuiteBuilder}.",,," Can't be invoked directly;
 * please see {@link com.google.common.collect.testing.MapTestSuiteBuilder}.",,,,,,
E3,E1,,,CollectionTestSuiteBuilder.java,"
 * Concrete instantiation of {@link AbstractCollectionTestSuiteBuilder} for testing collections that
 * do not have a more specific tester like {@link ListTestSuiteBuilder} or {@link
 * SetTestSuiteBuilder}.
 *
 * @author Chris Povirk
 * @author Louis Wasserman
 ",8," * Concrete instantiation of {@link AbstractCollectionTestSuiteBuilder} for testing collections that
 * do not have a more specific tester like {@link ListTestSuiteBuilder} or {@link
 * SetTestSuiteBuilder}.",,,,,,,,,,,," * @author Chris Povirk
 * @author Louis Wasserman","@link AbstractCollectionTestSuiteBuilder} 
{@link ListTestSuiteBuilder} or {@link
 * SetTestSuiteBuilder}.",,,,,,,,,
E3,E2,,,PeekingIterator.java,"
 * An iterator that supports a one-element lookahead while iterating.
 *
 * <p>See the Guava User Guide article on <a href=
 * ""https://github.com/google/guava/wiki/CollectionHelpersExplained#peekingiterator""> {@code
 * PeekingIterator}</a>.
 *
 * @author Mick Killianey
 * @since 2.0",10,"
 * An iterator that supports a one-element lookahead while iterating.",,,,,,,,,,,," * @author Mick Killianey
 * @since 2.0"," * <p>See the Guava User Guide article on <a href=
 * ""https://github.com/google/guava/wiki/CollectionHelpersExplained#peekingiterator""> {@code
 * PeekingIterator}</a>.",,,,,,,,,
E3,E2,,,SmoothRateLimiter.java,"
   * How is the RateLimiter designed, and why?
   *
   * The primary feature of a RateLimiter is its ""stable rate"", the maximum rate that is should
   * allow at normal conditions. This is enforced by ""throttling"" incoming requests as needed, i.e.
   * compute, for an incoming request, the appropriate throttle time, and make the calling thread
   * wait as much.
   *
   * The simplest way to maintain a rate of QPS is to keep the timestamp of the last granted
   * request, and ensure that (1/QPS) seconds have elapsed since then. For example, for a rate of
   * QPS=5 (5 tokens per second), if we ensure that a request isn't granted earlier than 200ms after
   * the last one, then we achieve the intended rate. If a request comes and the last request was
   * granted only 100ms ago, then we wait for another 100ms. At this rate, serving 15 fresh permits
   * (i.e. for an acquire(15) request) naturally takes 3 seconds.
   *
   * It is important to realize that such a RateLimiter has a very superficial memory of the past:
   * it only remembers the last request. What if the RateLimiter was unused for a long period of
   * time, then a request arrived and was immediately granted? This RateLimiter would immediately
   * forget about that past underutilization. This may result in either underutilization or
   * overflow, depending on the real world consequences of not using the expected rate.
   *
   * Past underutilization could mean that excess resources are available. Then, the RateLimiter
   * should speed up for a while, to take advantage of these resources. This is important when the
   * rate is applied to networking (limiting bandwidth), where past underutilization typically
   * translates to ""almost empty buffers"", which can be filled immediately.
   *
   * On the other hand, past underutilization could mean that ""the server responsible for handling
   * the request has become less ready for future requests"", i.e. its caches become stale, and
   * requests become more likely to trigger expensive operations (a more extreme case of this
   * example is when a server has just booted, and it is mostly busy with getting itself up to
   * speed).
   *
   * To deal with such scenarios, we add an extra dimension, that of ""past underutilization"",
   * modeled by ""storedPermits"" variable. This variable is zero when there is no underutilization,
   * and it can grow up to maxStoredPermits, for sufficiently large underutilization. So, the
   * requested permits, by an invocation acquire(permits), are served from:
   *
   * - stored permits (if available)
   *
   * - fresh permits (for any remaining permits)
   *
   * How this works is best explained with an example:
   *
   * For a RateLimiter that produces 1 token per second, every second that goes by with the
   * RateLimiter being unused, we increase storedPermits by 1. Say we leave the RateLimiter unused
   * for 10 seconds (i.e., we expected a request at time X, but we are at time X + 10 seconds before
   * a request actually arrives; this is also related to the point made in the last paragraph), thus
   * storedPermits becomes 10.0 (assuming maxStoredPermits >= 10.0). At that point, a request of
   * acquire(3) arrives. We serve this request out of storedPermits, and reduce that to 7.0 (how
   * this is translated to throttling time is discussed later). Immediately after, assume that an
   * acquire(10) request arriving. We serve the request partly from storedPermits, using all the
   * remaining 7.0 permits, and the remaining 3.0, we serve them by fresh permits produced by the
   * rate limiter.
   *
   * We already know how much time it takes to serve 3 fresh permits: if the rate is
   * ""1 token per second"", then this will take 3 seconds. But what does it mean to serve 7 stored
   * permits? As explained above, there is no unique answer. If we are primarily interested to deal
   * with underutilization, then we want stored permits to be given out /faster/ than fresh ones,
   * because underutilization = free resources for the taking. If we are primarily interested to
   * deal with overflow, then stored permits could be given out /slower/ than fresh ones. Thus, we
   * require a (different in each case) function that translates storedPermits to throttling time.
   *
   * This role is played by storedPermitsToWaitTime(double storedPermits, double permitsToTake). The
   * underlying model is a continuous function mapping storedPermits (from 0.0 to maxStoredPermits)
   * onto the 1/rate (i.e. intervals) that is effective at the given storedPermits. ""storedPermits""
   * essentially measure unused time; we spend unused time buying/storing permits. Rate is
   * ""permits / time"", thus ""1 / rate = time / permits"". Thus, ""1/rate"" (time / permits) times
   * ""permits"" gives time, i.e., integrals on this function (which is what storedPermitsToWaitTime()
   * computes) correspond to minimum intervals between subsequent requests, for the specified number
   * of requested permits.
   *
   * Here is an example of storedPermitsToWaitTime: If storedPermits == 10.0, and we want 3 permits,
   * we take them from storedPermits, reducing them to 7.0, and compute the throttling for these as
   * a call to storedPermitsToWaitTime(storedPermits = 10.0, permitsToTake = 3.0), which will
   * evaluate the integral of the function from 7.0 to 10.0.
   *
   * Using integrals guarantees that the effect of a single acquire(3) is equivalent to {
   * acquire(1); acquire(1); acquire(1); }, or { acquire(2); acquire(1); }, etc, since the integral
   * of the function in [7.0, 10.0] is equivalent to the sum of the integrals of [7.0, 8.0], [8.0,
   * 9.0], [9.0, 10.0] (and so on), no matter what the function is. This guarantees that we handle
   * correctly requests of varying weight (permits), /no matter/ what the actual function is - so we
   * can tweak the latter freely. (The only requirement, obviously, is that we can compute its
   * integrals).
   *
   * Note well that if, for this function, we chose a horizontal line, at height of exactly (1/QPS),
   * then the effect of the function is non-existent: we serve storedPermits at exactly the same
   * cost as fresh ones (1/QPS is the cost for each). We use this trick later.
   *
   * If we pick a function that goes /below/ that horizontal line, it means that we reduce the area
   * of the function, thus time. Thus, the RateLimiter becomes /faster/ after a period of
   * underutilization. If, on the other hand, we pick a function that goes /above/ that horizontal
   * line, then it means that the area (time) is increased, thus storedPermits are more costly than
   * fresh permits, thus the RateLimiter becomes /slower/ after a period of underutilization.
   *
   * Last, but not least: consider a RateLimiter with rate of 1 permit per second, currently
   * completely unused, and an expensive acquire(100) request comes. It would be nonsensical to just
   * wait for 100 seconds, and /then/ start the actual task. Why wait without doing anything? A much
   * better approach is to /allow/ the request right away (as if it was an acquire(1) request
   * instead), and postpone /subsequent/ requests as needed. In this version, we allow starting the
   * task immediately, and postpone by 100 seconds future requests, thus we allow for work to get
   * done in the meantime instead of waiting idly.
   *
   * This has important consequences: it means that the RateLimiter doesn't remember the time of the
   * _last_ request, but it remembers the (expected) time of the _next_ request. This also enables
   * us to tell immediately (see tryAcquire(timeout)) whether a particular timeout is enough to get
   * us to the point of the next scheduling time, since we always maintain that. And what we mean by
   * ""an unused RateLimiter"" is also defined by that notion: when we observe that the
   * ""expected arrival time of the next request"" is actually in the past, then the difference (now -
   * past) is the amount of time that the RateLimiter was formally unused, and it is that amount of
   * time which we translate to storedPermits. (We increase storedPermits with the amount of permits
   * that would have been produced in that idle time). So, if rate == 1 permit per second, and
   * arrivals come exactly one second after the previous, then storedPermits is _never_ increased --
   * we would only increase it for arrivals _later_ than the expected one second.
   |
   * The time when the next request (no matter its size) will be granted. After granting a request,
   * this is pushed further in the future. Large requests push this further than small requests.
   |
   * This implements the following function where coldInterval = coldFactor * stableInterval.
   *
   * <pre>
   *          ^ throttling
   *          |
   *    cold  +                  /
   * interval |                 /.
   *          |                / .
   *          |               /  .   ← ""warmup period"" is the area of the trapezoid between
   *          |              /   .     thresholdPermits and maxPermits
   *          |             /    .
   *          |            /     .
   *          |           /      .
   *   stable +----------/  WARM .
   * interval |          .   UP  .
   *          |          . PERIOD.
   *          |          .       .
   *        0 +----------+-------+--------------→ storedPermits
   *          0 thresholdPermits maxPermits
   * </pre>
   *
   * Before going into the details of this particular function, let's keep in mind the basics:
   *
   * <ol>
   *   <li>The state of the RateLimiter (storedPermits) is a vertical line in this figure.
   *   <li>When the RateLimiter is not used, this goes right (up to maxPermits)
   *   <li>When the RateLimiter is used, this goes left (down to zero), since if we have
   *       storedPermits, we serve from those first
   *   <li>When _unused_, we go right at a constant rate! The rate at which we move to the right is
   *       chosen as maxPermits / warmupPeriod. This ensures that the time it takes to go from 0 to
   *       maxPermits is equal to warmupPeriod.
   *   <li>When _used_, the time it takes, as explained in the introductory class note, is equal to
   *       the integral of our function, between X permits and X-K permits, assuming we want to
   *       spend K saved permits.
   * </ol>
   *
   * <p>In summary, the time it takes to move to the left (spend K permits), is equal to the area of
   * the function of width == K.
   *
   * <p>Assuming we have saturated demand, the time to go from maxPermits to thresholdPermits is
   * equal to warmupPeriod. And the time to go from thresholdPermits to 0 is warmupPeriod/2. (The
   * reason that this is warmupPeriod/2 is to maintain the behavior of the original implementation
   * where coldFactor was hard coded as 3.)
   *
   * <p>It remains to calculate thresholdsPermits and maxPermits.
   *
   * <ul>
   *   <li>The time to go from thresholdPermits to 0 is equal to the integral of the function
   *       between 0 and thresholdPermits. This is thresholdPermits * stableIntervals. By (5) it is
   *       also equal to warmupPeriod/2. Therefore
   *       <blockquote>
   *       thresholdPermits = 0.5 * warmupPeriod / stableInterval
   *       </blockquote>
   *   <li>The time to go from maxPermits to thresholdPermits is equal to the integral of the
   *       function between thresholdPermits and maxPermits. This is the area of the pictured
   *       trapezoid, and it is equal to 0.5 * (stableInterval + coldInterval) * (maxPermits -
   *       thresholdPermits). It is also equal to warmupPeriod, so
   *       <blockquote>
   *       maxPermits = thresholdPermits + 2 * warmupPeriod / (stableInterval + coldInterval)
   *       </blockquote>
   * </ul>
   |
   * How is the RateLimiter designed, and why?
   *
   * The primary feature of a RateLimiter is its ""stable rate"", the maximum rate that is should
   * allow at normal conditions. This is enforced by ""throttling"" incoming requests as needed, i.e.
   * compute, for an incoming request, the appropriate throttle time, and make the calling thread
   * wait as much.
   *
   * The simplest way to maintain a rate of QPS is to keep the timestamp of the last granted
   * request, and ensure that (1/QPS) seconds have elapsed since then. For example, for a rate of
   * QPS=5 (5 tokens per second), if we ensure that a request isn't granted earlier than 200ms after
   * the last one, then we achieve the intended rate. If a request comes and the last request was
   * granted only 100ms ago, then we wait for another 100ms. At this rate, serving 15 fresh permits
   * (i.e. for an acquire(15) request) naturally takes 3 seconds.
   *
   * It is important to realize that such a RateLimiter has a very superficial memory of the past:
   * it only remembers the last request. What if the RateLimiter was unused for a long period of
   * time, then a request arrived and was immediately granted? This RateLimiter would immediately
   * forget about that past underutilization. This may result in either underutilization or
   * overflow, depending on the real world consequences of not using the expected rate.
   *
   * Past underutilization could mean that excess resources are available. Then, the RateLimiter
   * should speed up for a while, to take advantage of these resources. This is important when the
   * rate is applied to networking (limiting bandwidth), where past underutilization typically
   * translates to ""almost empty buffers"", which can be filled immediately.
   *
   * On the other hand, past underutilization could mean that ""the server responsible for handling
   * the request has become less ready for future requests"", i.e. its caches become stale, and
   * requests become more likely to trigger expensive operations (a more extreme case of this
   * example is when a server has just booted, and it is mostly busy with getting itself up to
   * speed).
   *
   * To deal with such scenarios, we add an extra dimension, that of ""past underutilization"",
   * modeled by ""storedPermits"" variable. This variable is zero when there is no underutilization,
   * and it can grow up to maxStoredPermits, for sufficiently large underutilization. So, the
   * requested permits, by an invocation acquire(permits), are served from:
   *
   * - stored permits (if available)
   *
   * - fresh permits (for any remaining permits)
   *
   * How this works is best explained with an example:
   *
   * For a RateLimiter that produces 1 token per second, every second that goes by with the
   * RateLimiter being unused, we increase storedPermits by 1. Say we leave the RateLimiter unused
   * for 10 seconds (i.e., we expected a request at time X, but we are at time X + 10 seconds before
   * a request actually arrives; this is also related to the point made in the last paragraph), thus
   * storedPermits becomes 10.0 (assuming maxStoredPermits >= 10.0). At that point, a request of
   * acquire(3) arrives. We serve this request out of storedPermits, and reduce that to 7.0 (how
   * this is translated to throttling time is discussed later). Immediately after, assume that an
   * acquire(10) request arriving. We serve the request partly from storedPermits, using all the
   * remaining 7.0 permits, and the remaining 3.0, we serve them by fresh permits produced by the
   * rate limiter.
   *
   * We already know how much time it takes to serve 3 fresh permits: if the rate is
   * ""1 token per second"", then this will take 3 seconds. But what does it mean to serve 7 stored
   * permits? As explained above, there is no unique answer. If we are primarily interested to deal
   * with underutilization, then we want stored permits to be given out /faster/ than fresh ones,
   * because underutilization = free resources for the taking. If we are primarily interested to
   * deal with overflow, then stored permits could be given out /slower/ than fresh ones. Thus, we
   * require a (different in each case) function that translates storedPermits to throttling time.
   *
   * This role is played by storedPermitsToWaitTime(double storedPermits, double permitsToTake). The
   * underlying model is a continuous function mapping storedPermits (from 0.0 to maxStoredPermits)
   * onto the 1/rate (i.e. intervals) that is effective at the given storedPermits. ""storedPermits""
   * essentially measure unused time; we spend unused time buying/storing permits. Rate is
   * ""permits / time"", thus ""1 / rate = time / permits"". Thus, ""1/rate"" (time / permits) times
   * ""permits"" gives time, i.e., integrals on this function (which is what storedPermitsToWaitTime()
   * computes) correspond to minimum intervals between subsequent requests, for the specified number
   * of requested permits.
   *
   * Here is an example of storedPermitsToWaitTime: If storedPermits == 10.0, and we want 3 permits,
   * we take them from storedPermits, reducing them to 7.0, and compute the throttling for these as
   * a call to storedPermitsToWaitTime(storedPermits = 10.0, permitsToTake = 3.0), which will
   * evaluate the integral of the function from 7.0 to 10.0.
   *
   * Using integrals guarantees that the effect of a single acquire(3) is equivalent to {
   * acquire(1); acquire(1); acquire(1); }, or { acquire(2); acquire(1); }, etc, since the integral
   * of the function in [7.0, 10.0] is equivalent to the sum of the integrals of [7.0, 8.0], [8.0,
   * 9.0], [9.0, 10.0] (and so on), no matter what the function is. This guarantees that we handle
   * correctly requests of varying weight (permits), /no matter/ what the actual function is - so we
   * can tweak the latter freely. (The only requirement, obviously, is that we can compute its
   * integrals).
   *
   * Note well that if, for this function, we chose a horizontal line, at height of exactly (1/QPS),
   * then the effect of the function is non-existent: we serve storedPermits at exactly the same
   * cost as fresh ones (1/QPS is the cost for each). We use this trick later.
   *
   * If we pick a function that goes /below/ that horizontal line, it means that we reduce the area
   * of the function, thus time. Thus, the RateLimiter becomes /faster/ after a period of
   * underutilization. If, on the other hand, we pick a function that goes /above/ that horizontal
   * line, then it means that the area (time) is increased, thus storedPermits are more costly than
   * fresh permits, thus the RateLimiter becomes /slower/ after a period of underutilization.
   *
   * Last, but not least: consider a RateLimiter with rate of 1 permit per second, currently
   * completely unused, and an expensive acquire(100) request comes. It would be nonsensical to just
   * wait for 100 seconds, and /then/ start the actual task. Why wait without doing anything? A much
   * better approach is to /allow/ the request right away (as if it was an acquire(1) request
   * instead), and postpone /subsequent/ requests as needed. In this version, we allow starting the
   * task immediately, and postpone by 100 seconds future requests, thus we allow for work to get
   * done in the meantime instead of waiting idly.
   *
   * This has important consequences: it means that the RateLimiter doesn't remember the time of the
   * _last_ request, but it remembers the (expected) time of the _next_ request. This also enables
   * us to tell immediately (see tryAcquire(timeout)) whether a particular timeout is enough to get
   * us to the point of the next scheduling time, since we always maintain that. And what we mean by
   * ""an unused RateLimiter"" is also defined by that notion: when we observe that the
   * ""expected arrival time of the next request"" is actually in the past, then the difference (now -
   * past) is the amount of time that the RateLimiter was formally unused, and it is that amount of
   * time which we translate to storedPermits. (We increase storedPermits with the amount of permits
   * that would have been produced in that idle time). So, if rate == 1 permit per second, and
   * arrivals come exactly one second after the previous, then storedPermits is _never_ increased --
   * we would only increase it for arrivals _later_ than the expected one second.
   |
   * The time when the next request (no matter its size) will be granted. After granting a request,
   * this is pushed further in the future. Large requests push this further than small requests.
   |
   * This implements a ""bursty"" RateLimiter, where storedPermits are translated to zero throttling.
   * The maximum number of permits that can be saved (when the RateLimiter is unused) is defined in
   * terms of time, in this sense: if a RateLimiter is 2qps, and this time is specified as 10
   * seconds, we can save up to 2 * 10 = 20 permits.
   ",300,"   * The primary feature of a RateLimiter is its ""stable rate"", the maximum rate that is should
   * allow at normal conditions. This is enforced by ""throttling"" incoming requests as needed, i.e.
   * compute, for an incoming request, the appropriate throttle time, and make the calling thread
   * wait as much.
   * <p>In summary, the time it takes to move to the left (spend K permits), is equal to the area of
   * the function of width == K","   * The simplest way to maintain a rate of QPS is to keep the timestamp of the last granted
   * request, and ensure that (1/QPS) seconds have elapsed since then. For example, for a rate of
   * QPS=5 (5 tokens per second), if we ensure that a request isn't granted earlier than 200ms after
   * the last one, then we achieve the intended rate. If a request comes and the last request was
   * granted only 100ms ago, then we wait for another 100ms. At this rate, serving 15 fresh permits
   * (i.e. for an acquire(15) request) naturally takes 3 seconds.
   *
   * It is important to realize that such a RateLimiter has a very superficial memory of the past:
   * it only remembers the last request. What if the RateLimiter was unused for a long period of
   * time, then a request arrived and was immediately granted? This RateLimiter would immediately
   * forget about that past underutilization. This may result in either underutilization or
   * overflow, depending on the real world consequences of not using the expected rate.
   *
   * Past underutilization could mean that excess resources are available. Then, the RateLimiter
   * should speed up for a while, to take advantage of these resources. This is important when the
   * rate is applied to networking (limiting bandwidth), where past underutilization typically
   * translates to ""almost empty buffers"", which can be filled immediately.
   *
   * On the other hand, past underutilization could mean that ""the server responsible for handling
   * the request has become less ready for future requests"", i.e. its caches become stale, and
   * requests become more likely to trigger expensive operations (a more extreme case of this
   * example is when a server has just booted, and it is mostly busy with getting itself up to
   * speed).
   *
   * To deal with such scenarios, we add an extra dimension, that of ""past underutilization"",
   * modeled by ""storedPermits"" variable. This variable is zero when there is no underutilization,
   * and it can grow up to maxStoredPermits, for sufficiently large underutilization. So, the
   * requested permits, by an invocation acquire(permits), are served from:
   *
   * - stored permits (if available)
   *
   * - fresh permits (for any remaining permits)
 
   * This implements the following function where coldInterval = coldFactor * stableInterval.
   *
   * <pre>
   *          ^ throttling
   *          |
   *    cold  +                  /
   * interval |                 /.
   *          |                / .
   *          |               /  .   ← ""warmup period"" is the area of the trapezoid between
   *          |              /   .     thresholdPermits and maxPermits
   *          |             /    .
   *          |            /     .
   *          |           /      .
   *   stable +----------/  WARM .
   * interval |          .   UP  .
   *          |          . PERIOD.
   *          |          .       .
   *        0 +----------+-------+--------------→ storedPermits
   *          0 thresholdPermits maxPermits
   * </pre>
   *
   * Before going into the details of this particular function, let's keep in mind the basics:
   *
   * <ol>
   *   <li>The state of the RateLimiter (storedPermits) is a vertical line in this figure.
   *   <li>When the RateLimiter is not used, this goes right (up to maxPermits)
   *   <li>When the RateLimiter is used, this goes left (down to zero), since if we have
   *       storedPermits, we serve from those first
   *   <li>When _unused_, we go right at a constant rate! The rate at which we move to the right is
   *       chosen as maxPermits / warmupPeriod. This ensures that the time it takes to go from 0 to
   *       maxPermits is equal to warmupPeriod.
   *   <li>When _used_, the time it takes, as explained in the introductory class note, is equal to
   *       the integral of our function, between X permits and X-K permits, assuming we want to
   *       spend K saved permits.
   * </ol>","   |
   * This implements a ""bursty"" RateLimiter, where storedPermits are translated to zero throttling.
   * The maximum number of permits that can be saved (when the RateLimiter is unused) is defined in
   * terms of time, in this sense: if a RateLimiter is 2qps, and this time is specified as 10
   * seconds, we can save up to 2 * 10 = 20 permits.
   * It is important to realize that such a RateLimiter has a very superficial memory of the past:
   * it only remembers the last request. What if the RateLimiter was unused for a long period of
   * time, then a request arrived and was immediately granted? This RateLimiter would immediately
   * forget about that past underutilization. This may result in either underutilization or
   * overflow, depending on the real world consequences of not using the expected rate.",," How this works is best explained with an example:
   *
   * For a RateLimiter that produces 1 token per second, every second that goes by with the
   * RateLimiter being unused, we increase storedPermits by 1. Say we leave the RateLimiter unused
   * for 10 seconds (i.e., we expected a request at time X, but we are at time X + 10 seconds before
   * a request actually arrives; this is also related to the point made in the last paragraph), thus
   * storedPermits becomes 10.0 (assuming maxStoredPermits >= 10.0). At that point, a request of
   * acquire(3) arrives. We serve this request out of storedPermits, and reduce that to 7.0 (how
   * this is translated to throttling time is discussed later). Immediately after, assume that an
   * acquire(10) request arriving. We serve the request partly from storedPermits, using all the
   * remaining 7.0 permits, and the remaining 3.0, we serve them by fresh permits produced by the
   * rate limiter.
   *
   * We already know how much time it takes to serve 3 fresh permits: if the rate is
   * ""1 token per second"", then this will take 3 seconds. But what does it mean to serve 7 stored
   * permits? As explained above, there is no unique answer. If we are primarily interested to deal
   * with underutilization, then we want stored permits to be given out /faster/ than fresh ones,
   * because underutilization = free resources for the taking. If we are primarily interested to
   * deal with overflow, then stored permits could be given out /slower/ than fresh ones. Thus, we
   * require a (different in each case) function that translates storedPermits to throttling time.
   *
   * This role is played by storedPermitsToWaitTime(double storedPermits, double permitsToTake). The
   * underlying model is a continuous function mapping storedPermits (from 0.0 to maxStoredPermits)
   * onto the 1/rate (i.e. intervals) that is effective at the given storedPermits. ""storedPermits""
   * essentially measure unused time; we spend unused time buying/storing permits. Rate is
   * ""permits / time"", thus ""1 / rate = time / permits"". Thus, ""1/rate"" (time / permits) times
   * ""permits"" gives time, i.e., integrals on this function (which is what storedPermitsToWaitTime()
   * computes) correspond to minimum intervals between subsequent requests, for the specified number
   * of requested permits.
   *
   * Here is an example of storedPermitsToWaitTime: If storedPermits == 10.0, and we want 3 permits,
   * we take them from storedPermits, reducing them to 7.0, and compute the throttling for these as
   * a call to storedPermitsToWaitTime(storedPermits = 10.0, permitsToTake = 3.0), which will
   * evaluate the integral of the function from 7.0 to 10.0.
   *
   * Using integrals guarantees that the effect of a single acquire(3) is equivalent to {
   * acquire(1); acquire(1); acquire(1); }, or { acquire(2); acquire(1); }, etc, since the integral
   * of the function in [7.0, 10.0] is equivalent to the sum of the integrals of [7.0, 8.0], [8.0,
   * 9.0], [9.0, 10.0] (and so on), no matter what the function is. This guarantees that we handle
   * correctly requests of varying weight (permits), /no matter/ what the actual function is - so we
   * can tweak the latter freely. (The only requirement, obviously, is that we can compute its
   * integrals).
If we pick a function that goes /below/ that horizontal line, it means that we reduce the area
   * of the function, thus time. Thus, the RateLimiter becomes /faster/ after a period of
   * underutilization. If, on the other hand, we pick a function that goes /above/ that horizontal
   * line, then it means that the area (time) is increased, thus storedPermits are more costly than
   * fresh permits, thus the RateLimiter becomes /slower/ after a period of underutilization.
   *
   * Last, but not least: consider a RateLimiter with rate of 1 permit per second, currently
   * completely unused, and an expensive acquire(100) request comes. It would be nonsensical to just
   * wait for 100 seconds, and /then/ start the actual task. Why wait without doing anything? A much
   * better approach is to /allow/ the request right away (as if it was an acquire(1) request
   * instead), and postpone /subsequent/ requests as needed. In this version, we allow starting the
   * task immediately, and postpone by 100 seconds future requests, thus we allow for work to get
   * done in the meantime instead of waiting idly.
   *
   * This has important consequences: it means that the RateLimiter doesn't remember the time of the
   * _last_ request, but it remembers the (expected) time of the _next_ request. This also enables
   * us to tell immediately (see tryAcquire(timeout)) whether a particular timeout is enough to get
   * us to the point of the next scheduling time, since we always maintain that. And what we mean by
   * ""an unused RateLimiter"" is also defined by that notion: when we observe that the
   * ""expected arrival time of the next request"" is actually in the past, then the difference (now -
   * past) is the amount of time that the RateLimiter was formally unused, and it is that amount of
   * time which we translate to storedPermits. (We increase storedPermits with the amount of permits
   * that would have been produced in that idle time). So, if rate == 1 permit per second, and
   * arrivals come exactly one second after the previous, then storedPermits is _never_ increased --
   * we would only increase it for arrivals _later_ than the expected one second.
   |
   * The time when the next request (no matter its size) will be granted. After granting a request,
   * this is pushed further in the future. Large requests push this further than small requests.
   |",,,,,,,,,,"   * How is the RateLimiter designed, and why?
   *
   * The primary feature of a RateLimiter is its ""stable rate"", the maximum rate that is should
   * allow at normal conditions. This is enforced by ""throttling"" incoming requests as needed, i.e.
   * compute, for an incoming request, the appropriate throttle time, and make the calling thread
   * wait as much.
   *
   * The simplest way to maintain a rate of QPS is to keep the timestamp of the last granted
   * request, and ensure that (1/QPS) seconds have elapsed since then. For example, for a rate of
   * QPS=5 (5 tokens per second), if we ensure that a request isn't granted earlier than 200ms after
   * the last one, then we achieve the intended rate. If a request comes and the last request was
   * granted only 100ms ago, then we wait for another 100ms. At this rate, serving 15 fresh permits
   * (i.e. for an acquire(15) request) naturally takes 3 seconds.
   *
   * It is important to realize that such a RateLimiter has a very superficial memory of the past:
   * it only remembers the last request. What if the RateLimiter was unused for a long period of
   * time, then a request arrived and was immediately granted? This RateLimiter would immediately
   * forget about that past underutilization. This may result in either underutilization or
   * overflow, depending on the real world consequences of not using the expected rate.
   *
   * Past underutilization could mean that excess resources are available. Then, the RateLimiter
   * should speed up for a while, to take advantage of these resources. This is important when the
   * rate is applied to networking (limiting bandwidth), where past underutilization typically
   * translates to ""almost empty buffers"", which can be filled immediately.
   *
   * On the other hand, past underutilization could mean that ""the server responsible for handling
   * the request has become less ready for future requests"", i.e. its caches become stale, and
   * requests become more likely to trigger expensive operations (a more extreme case of this
   * example is when a server has just booted, and it is mostly busy with getting itself up to
   * speed).
   *
   * To deal with such scenarios, we add an extra dimension, that of ""past underutilization"",
   * modeled by ""storedPermits"" variable. This variable is zero when there is no underutilization,
   * and it can grow up to maxStoredPermits, for sufficiently large underutilization. So, the
   * requested permits, by an invocation acquire(permits), are served from:
   *
   * - stored permits (if available)
   *
   * - fresh permits (for any remaining permits)
   *
   * How this works is best explained with an example:
   *
   * For a RateLimiter that produces 1 token per second, every second that goes by with the
   * RateLimiter being unused, we increase storedPermits by 1. Say we leave the RateLimiter unused
   * for 10 seconds (i.e., we expected a request at time X, but we are at time X + 10 seconds before
   * a request actually arrives; this is also related to the point made in the last paragraph), thus
   * storedPermits becomes 10.0 (assuming maxStoredPermits >= 10.0). At that point, a request of
   * acquire(3) arrives. We serve this request out of storedPermits, and reduce that to 7.0 (how
   * this is translated to throttling time is discussed later). Immediately after, assume that an
   * acquire(10) request arriving. We serve the request partly from storedPermits, using all the
   * remaining 7.0 permits, and the remaining 3.0, we serve them by fresh permits produced by the
   * rate limiter.
   *
   * We already know how much time it takes to serve 3 fresh permits: if the rate is
   * ""1 token per second"", then this will take 3 seconds. But what does it mean to serve 7 stored
   * permits? As explained above, there is no unique answer. If we are primarily interested to deal
   * with underutilization, then we want stored permits to be given out /faster/ than fresh ones,
   * because underutilization = free resources for the taking. If we are primarily interested to
   * deal with overflow, then stored permits could be given out /slower/ than fresh ones. Thus, we
   * require a (different in each case) function that translates storedPermits to throttling time.
   *
   * This role is played by storedPermitsToWaitTime(double storedPermits, double permitsToTake). The
   * underlying model is a continuous function mapping storedPermits (from 0.0 to maxStoredPermits)
   * onto the 1/rate (i.e. intervals) that is effective at the given storedPermits. ""storedPermits""
   * essentially measure unused time; we spend unused time buying/storing permits. Rate is
   * ""permits / time"", thus ""1 / rate = time / permits"". Thus, ""1/rate"" (time / permits) times
   * ""permits"" gives time, i.e., integrals on this function (which is what storedPermitsToWaitTime()
   * computes) correspond to minimum intervals between subsequent requests, for the specified number
   * of requested permits.
   *
   * Here is an example of storedPermitsToWaitTime: If storedPermits == 10.0, and we want 3 permits,
   * we take them from storedPermits, reducing them to 7.0, and compute the throttling for these as
   * a call to storedPermitsToWaitTime(storedPermits = 10.0, permitsToTake = 3.0), which will
   * evaluate the integral of the function from 7.0 to 10.0.
   *
   * Using integrals guarantees that the effect of a single acquire(3) is equivalent to {
   * acquire(1); acquire(1); acquire(1); }, or { acquire(2); acquire(1); }, etc, since the integral
   * of the function in [7.0, 10.0] is equivalent to the sum of the integrals of [7.0, 8.0], [8.0,
   * 9.0], [9.0, 10.0] (and so on), no matter what the function is. This guarantees that we handle
   * correctly requests of varying weight (permits), /no matter/ what the actual function is - so we
   * can tweak the latter freely. (The only requirement, obviously, is that we can compute its
   * integrals).
   *
   * Note well that if, for this function, we chose a horizontal line, at height of exactly (1/QPS),
   * then the effect of the function is non-existent: we serve storedPermits at exactly the same
   * cost as fresh ones (1/QPS is the cost for each). We use this trick later.
   *
   * If we pick a function that goes /below/ that horizontal line, it means that we reduce the area
   * of the function, thus time. Thus, the RateLimiter becomes /faster/ after a period of
   * underutilization. If, on the other hand, we pick a function that goes /above/ that horizontal
   * line, then it means that the area (time) is increased, thus storedPermits are more costly than
   * fresh permits, thus the RateLimiter becomes /slower/ after a period of underutilization.
   *
   * Last, but not least: consider a RateLimiter with rate of 1 permit per second, currently
   * completely unused, and an expensive acquire(100) request comes. It would be nonsensical to just
   * wait for 100 seconds, and /then/ start the actual task. Why wait without doing anything? A much
   * better approach is to /allow/ the request right away (as if it was an acquire(1) request
   * instead), and postpone /subsequent/ requests as needed. In this version, we allow starting the
   * task immediately, and postpone by 100 seconds future requests, thus we allow for work to get
   * done in the meantime instead of waiting idly.
   *
   * This has important consequences: it means that the RateLimiter doesn't remember the time of the
   * _last_ request, but it remembers the (expected) time of the _next_ request. This also enables
   * us to tell immediately (see tryAcquire(timeout)) whether a particular timeout is enough to get
   * us to the point of the next scheduling time, since we always maintain that. And what we mean by
   * ""an unused RateLimiter"" is also defined by that notion: when we observe that the
   * ""expected arrival time of the next request"" is actually in the past, then the difference (now -
   * past) is the amount of time that the RateLimiter was formally unused, and it is that amount of
   * time which we translate to storedPermits. (We increase storedPermits with the amount of permits
   * that would have been produced in that idle time). So, if rate == 1 permit per second, and
   * arrivals come exactly one second after the previous, then storedPermits is _never_ increased --
   * we would only increase it for arrivals _later_ than the expected one second.
   |
   * The time when the next request (no matter its size) will be granted. After granting a request,
   * this is pushed further in the future. Large requests push this further than small requests.",,"  * Note well that if, for this function, we chose a horizontal line, at height of exactly (1/QPS),
   * then the effect of the function is non-existent: we serve storedPermits at exactly the same
   * cost as fresh ones (1/QPS is the cost for each). We use this trick later.",," Thus, we
   * require a (different in each case) function that translates storedPermits to throttling time.
 (The only requirement, obviously, is that we can compute its
   * integrals).",,,,
E3,E2,,,BloomFilter.java,"
 * A Bloom filter for instances of {@code T}. A Bloom filter offers an approximate containment test
 * with one-sided error: if it claims that an element is contained in it, this might be in error,
 * but if it claims that an element is <i>not</i> contained in it, then this is definitely true.
 *
 * <p>If you are unfamiliar with Bloom filters, this nice <a
 * href=""http://llimllib.github.com/bloomfilter-tutorial/"">tutorial</a> may help you understand how
 * they work.
 *
 * <p>The false positive probability ({@code FPP}) of a Bloom filter is defined as the probability
 * that {@linkplain #mightContain(Object)} will erroneously return {@code true} for an object that
 * has not actually been put in the {@code BloomFilter}.
 *
 * <p>Bloom filters are serializable. They also support a more compact serial representation via the
 * {@link #writeTo} and {@link #readFrom} methods. Both serialized forms will continue to be
 * supported by future versions of this library. However, serial forms generated by newer versions
 * of the code may not be readable by older versions of the code (e.g., a serialized Bloom filter
 * generated today may <i>not</i> be readable by a binary that was compiled 6 months ago).
 *
 * <p>As of Guava 23.0, this class is thread-safe and lock-free. It internally uses atomics and
 * compare-and-swap to ensure correctness when multiple threads are used to access it.
 *
 * @param <T> the type of instances that the {@code BloomFilter} accepts
 * @author Dimitris Andreou
 * @author Kevin Bourrillion
 * @since 11.0 (thread-safe since 23.0)
 | Cheat sheet:|| m: total bits| n: expected insertions| b: m/n, bits per insertion| p: expected false positive probability|| 1) Optimal k = b * ln2| 2) p = (1 - e ^ (-kn/m))^k| 3) For optimal k: p = 2 ^ (-k) ~= 0.6185^b| 4) For optimal k: m = -nlnp / ((ln2) ^ 2)|
   * A strategy to translate T instances, to {@code numHashFunctions} bit indexes.
   *
   * <p>Implementations should be collections of pure functions (i.e. stateless).
   ",31,"
 * A Bloom filter for instances of {@code T}. A Bloom filter offers an approximate containment test
 * with one-sided error: if it claims that an element is contained in it, this might be in error,
 * but if it claims that an element is <i>not</i> contained in it, then this is definitely true."," <p>The false positive probability ({@code FPP}) of a Bloom filter is defined as the probability
 * that {@linkplain #mightContain(Object)} will erroneously return {@code true} for an object that
 * has not actually been put in the {@code BloomFilter}.
 *
 * <p>Bloom filters are serializable. They also support a more compact serial representation via the
 * {@link #writeTo} and {@link #readFrom} methods. Both serialized forms will continue to be
 * supported by future versions of this library. However, serial forms generated by newer versions
 * of the code may not be readable by older versions of the code (e.g., a serialized Bloom filter
 * generated today may <i>not</i> be readable by a binary that was compiled 6 months ago).",," However, serial forms generated by newer versions
 * of the code may not be readable by older versions of the code (e.g., a serialized Bloom filter
 * generated today may <i>not</i> be readable by a binary that was compiled 6 months ago).
 * <p>As of Guava 23.0, this class is thread-safe and lock-free. It internally uses atomics and
 * compare-and-swap to ensure correctness when multiple threads are used to access it. 
* @since 11.0 (thread-safe since 23.0)
"," * @param <T> the type of instances that the {@code BloomFilter} accepts.
 | Cheat sheet:|| m: total bits| n: expected insertions| b: m/n, bits per insertion| p: expected false positive probability|| 1) Optimal k = b * ln2| 2) p = (1 - e ^ (-kn/m))^k| 3) For optimal k: p = 2 ^ (-k) ~= 0.6185^b| 4) For optimal k: m = -nlnp / ((ln2) ^ 2)|
   * A strategy to translate T instances, to {@code numHashFunctions} bit indexes.
   *
   * <p>Implementations should be collections of pure functions (i.e. stateless).",,,,,,,," * @author Dimitris Andreou
 * @author Kevin Bourrillion"," * <p>If you are unfamiliar with Bloom filters, this nice <a
 * href=""http://llimllib.github.com/bloomfilter-tutorial/"">tutorial</a> may help you understand how
 * they work.
 * <p>The false positive probability ({@code FPP}) of a Bloom filter is defined as the probability
 * that {@linkplain #mightContain(Object)} will erroneously return {@code true} for an object that
 * has not actually been put in the {@code BloomFilter}.
 *
 * <p>Bloom filters are serializable. They also support a more compact serial representation via the
 * {@link #writeTo} and {@link #readFrom} methods.",,,," * <p>If you are unfamiliar with Bloom filters, this nice <a
 * href=""http://llimllib.github.com/bloomfilter-tutorial/"">tutorial</a> may help you understand how
 * they work.
   * <p>Implementations should be collections of pure functions (i.e. stateless).",,,,," * <p>As of Guava 23.0, this class is thread-safe and lock-free. It internally uses atomics and
 * compare-and-swap to ensure correctness when multiple threads are used to access it."
E1,E2 (agree),,guava,LongMathBenchmark.java,"
 * Benchmarks for the non-rounding methods of {@code LongMath}.
 *
 * @author Louis Wasserman",5,* Benchmarks for the non-rounding methods of {@code LongMath}.,,,,,,,,,,,,@author Louis Wasserman,,,,,,,,,,
E1,E2 (agree),,,AbstractMultimapTester.java,"
 * Superclass for all {@code Multimap} testers.
 *
 * @author Louis Wasserman",5,* Superclass for all {@code Multimap} testers.,,,,,,,,,,,,@author Louis Wasserman,,,,,,,,,,
E1,E2 (agree),,,CacheManualTest.java, @author Charles Fry ,1,,,,,,,,,,,,,@author Charles Fry,,,,,,,,,,
E1,E2 (agree),,,GwtTestSuite.java,"
 * Runs all _gwt tests. Grouping them into a suite is much faster than running each as a one-test
 * ""suite,"" as the per-suite setup is expensive.",4,* Runs all _gwt tests.,,"Grouping them into a suite is much faster than running each as a one-test
  * ""suite,"" as the per-suite setup is expensive.",,,,,,,,,,,,,,,,,,,,
E1,E2 (agree),,,NullsLastOrdering_CustomFieldSerializer.java,"
 * This class implements the GWT serialization of {@link NullsLastOrdering}.
 *
 * @author Chris Povirk",5,* This class implements the GWT serialization of {@link NullsLastOrdering}.,,,,,,,,,,,,@author Chris Povirk,,,,,,,,,,
E1,E2 (agree),,,MultimapBuilderTest.java,"
 * Tests for {@link MultimapBuilder}.
 *
 * @author Louis Wasserman",5,* Tests for {@link MultimapBuilder}.,,,,,,,,,,,,@author Louis Wasserman,,,,,,,,,,
E1,E2 (agree),,,AtomicDoubleArrayTest.java,"* Source:
 * http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/tck-jsr166e/AtomicDoubleArrayTest.java?revision=1.13
 * (Modified to adapt to guava coding conventions)
 | Unit test for {@link AtomicDoubleArray}.",5,Unit test for {@link AtomicDoubleArray}.,,,,,,,,,,,,,"* Source:
  * http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/tck-jsr166e/AtomicDoubleArrayTest.java?revision=1.13
  * (Modified to adapt to guava coding conventions)",,,,,,,,,
E1,E2 (agree),,,AbstractMultimap.java,"
 * A skeleton {@code Multimap} implementation, not necessarily in terms of a {@code Map}.
 *
 * @author Louis Wasserman",5,"A skeleton {@code Multimap} implementation, not necessarily in terms of a {@code Map}.",,,,,,,,,,,,@author Louis Wasserman,,,,,,,,,,
E1,,E3 (agree),,MapSerializationTester.java,"
 * Basic serialization test for maps.
 *
 * @author Louis Wasserman",5,* Basic serialization test for maps.,,,,,,,,,,,,@author Louis Wasserman,,,,,,,,,,
E1,,E3 (agree),,TestStringListGenerator.java," * TODO: javadoc.
 *
 * @author Kevin Bourrillion",5,,,,,,,* TODO: javadoc.,,,,,,@author Kevin Bourrillion,,,,,,,,,,
E1,,E3 (agree),,HashFunctionEnum.java,"
 * An enum that contains all of the known hash functions.
 *
 * @author Kurt Alfred Kluever",5,An enum that contains all of the known hash functions.,,,,,,,,,,,,@author Kurt Alfred Kluever,,,,,,,,,,
E1,,E3 (agree),,ForwardingBlockingDequeTest.java," * Test for {@link ForwardingBlockingDeque}
 *
 * @author Emily Soldal",5,Test for {@link ForwardingBlockingDeque},,,,,,,,,,,,@author Emily Soldal,,,,,,,,,,
E1,,E3 (agree),,ConflictingRequirementsException.java,"
 * Thrown when requirements on a tester method or class conflict with each other.
 *
 * @author George van den Driessche",5,* Thrown when requirements on a tester method or class conflict with each other.,,,,,,,,,,,,@author George van den Driessche,,,,,,,,,,
E1,,E3 (agree),,BaseEncodingBenchmark.java, Benchmark for {@code BaseEncoding} performance. ,1,Benchmark for {@code BaseEncoding} performance.,,,,,,,,,,,,,,,,,,,,,,
E1,,E3 (agree),,MapSerializationTester.java,"
 * Basic serialization test for maps.
 *
 * @author Louis Wasserman",5,Basic serialization test for maps.,,,,,,,,,,,,* @author Louis Wasserman,,,,,,,,,,
E1,,E3 (agree),,CacheLoaderTest.java,"
 * Unit tests for {@link CacheLoader}.
 *
 * @author Charles Fry",5,* Unit tests for {@link CacheLoader}.,,,,,,,,,,,,@author Charles Fry,,,,,,,,,,
E1,,E3 (disagree),,LazyStackTraceBenchmark.java,"
 * Quick and dirty benchmark of {@link Throwables#lazyStackTrace(Throwable)}. We benchmark a ""caller
 * finder"" implementation that might be used in a logging framework.
 ",4,Quick and dirty benchmark of {@link Throwables#lazyStackTrace(Throwable)}.,,"We benchmark a ""caller
  * finder"" implementation that might be used in a logging framework.",,,,,,,,,,,Quick and dirty benchmark of {@link Throwables#lazyStackTrace(Throwable)}.,,,,,,,,,
E1,,E4 (agree),,CountingOutputStream.java,"* An OutputStream that counts the number of bytes written.
 *
 * @author Chris Nokleberg
 * @since 1.0",6,An OutputStream that counts the number of bytes written.,,,@since 1.0,,,,,,,,,@author Chris Nokleberg,,,,,,,,,,
E1,,E4 (agree),,ImmutableTable_CustomFieldSerializerBase.java,"
 * This class contains static utility methods for writing {@link ImmutableTable} GWT field
 * serializers. Serializers should delegate to {@link #serialize} and {@link #instantiate}.
 *
 * @author Chris Povirk",6,"* This class contains static utility methods for writing {@link ImmutableTable} GWT field
  * serializers.",,,,Serializers should delegate to {@link #serialize} and {@link #instantiate}.,,,,,,,,@author Chris Povirk,,,,,,,,,,
E1,,E4 (agree),,SortedSetNavigationTester.java,"
 * A generic JUnit test which tests operations on a SortedSet. Can't be invoked directly; please see
 * {@code SortedSetTestSuiteBuilder}.
 *
 * @author Jesse Wilson
 * @author Louis Wasserman",7,* A generic JUnit test which tests operations on a SortedSet.,,,,Can't be invoked directly;,,,,,,,,"* @author Jesse Wilson
  * @author Louis Wasserman","Can't be invoked directly; please see
  * {@code SortedSetTestSuiteBuilder}.",,,,,,,,,
E1,,E4 (agree),,TesterRequirements.java," * Encapsulates the constraints that a class under test must satisfy in order for a tester method to
 * be run against that class.
 *
 * @author George van den Driessche",6,"* Encapsulates the constraints that a class under test must satisfy in order for a tester method to
  * be run against that class.",,,,,,,,,,,,* @author George van den Driessche,,,,,,,,,,
E1,,E4 (agree),,ReserializingTestCollectionGenerator.java,"
 * Reserializes the sets created by another test set generator.
 *
 * <p>TODO: make CollectionTestSuiteBuilder test reserialized collections
 *
 * @author Jesse Wilson",7,* Reserializes the sets created by another test set generator.,,,,,,* <p>TODO: make CollectionTestSuiteBuilder test reserialized collections,,,,,,* @author Jesse Wilson,,,,,,,,,,
E1,,E4 (agree),,AbstractCollectionTester.java,"
 * Base class for collection testers.
 *
 * @param <E> the element type of the collection to be tested.
 * @author Kevin Bourrillion",6,* Base class for collection testers.,,,,* @param <E> the element type of the collection to be tested.,,,,,,,,* @author Kevin Bourrillion,,,,,,,,,,
E1,,E4 (agree),,SubscriberTest.java,"
 * Tests for {@link Subscriber}.
 *
 * @author Cliff Biffle
 * @author Colin Decker
 | Local exception subclass to check variety of exception thrown. | Local Error subclass to check variety of error thrown. ",6,* Tests for {@link Subscriber}.,,,,,| Local exception subclass to check variety of exception thrown. | Local Error subclass to check variety of error thrown.,,,,,,,"* @author Cliff Biffle
  * @author Colin Decker",,,,,,,,,,
E1,,E4 (agree),,ForwardingQueueTest.java,"
 * Tests for {@code ForwardingQueue}.
 *
 * @author Robert Konigsberg
 * @author Louis Wasserman",6,Tests for {@code ForwardingQueue}.,,,,,,,,,,,,"* @author Robert Konigsberg
  * @author Louis Wasserman",,,,,,,,,,
E1,E2 (agree),,,AbstractListeningExecutorService.java,"* Abstract {@link ListeningExecutorService} implementation that creates {@link ListenableFuture}
 * instances for each {@link Runnable} and {@link Callable} submitted to it. These tasks are run
 * with the abstract {@link #execute execute(Runnable)} method.
 *
 * <p>In addition to {@link #execute}, subclasses must implement all methods related to shutdown and
 * termination.
 *
 * @author Chris Povirk
 * @since 14.0",11,"* Abstract {@link ListeningExecutorService} implementation that creates {@link ListenableFuture}
  * instances for each {@link Runnable} and {@link Callable} submitted to it.","These tasks are run
  * with the abstract {@link #execute execute(Runnable)} method.",,* @since 14.0,"* <p>In addition to {@link #execute}, subclasses must implement all methods related to shutdown and
  * termination.",,,,,,,,* @author Chris Povirk,,,,,,,,,,
E1,E2 (agree),,,ObjectCountHashMap.java,"
 * ObjectCountHashMap is an implementation of {@code AbstractObjectCountMap} that uses arrays to
 * store key objects and count values. Comparing to using a traditional {@code HashMap}
 * implementation which stores keys and count values as map entries, {@code ObjectCountHashMap}
 * minimizes object allocation and reduces memory footprint.
 *
 * <p>In the absence of element deletions, this will iterate over elements in insertion order.
 ",8,"* ObjectCountHashMap is an implementation of {@code AbstractObjectCountMap} that uses arrays to
  * store key objects and count values.",,"Comparing to using a traditional {@code HashMap}
  * implementation which stores keys and count values as map entries, {@code ObjectCountHashMap}
  * minimizes object allocation and reduces memory footprint.",,"* <p>In the absence of element deletions, this will iterate over elements in insertion order.",,,,,,,,,,,,,,,,,,
E1,E2 (agree),,,WrappingScheduledExecutorService.java,"
 * An abstract {@code ScheduledExecutorService} that allows subclasses to {@linkplain
 * #wrapTask(Callable) wrap} tasks before they are submitted to the underlying executor.
 *
 * <p>Note that task wrapping may occur even if the task is never executed.
 *
 * @author Luke Sandberg",8,"* An abstract {@code ScheduledExecutorService} that allows subclasses to {@linkplain
  * #wrapTask(Callable) wrap} tasks before they are submitted to the underlying executor.",,,,* <p>Note that task wrapping may occur even if the task is never executed.,,,,,,,,* @author Luke Sandberg,,,,* <p>Note that task wrapping may occur even if the task is never executed.,,,,,,
E1,E2 (agree),,,AbstractSortedKeySortedSetMultimap.java,"
 * Basic implementation of a {@link SortedSetMultimap} with a sorted key set.
 *
 * <p>This superclass allows {@code TreeMultimap} to override methods to return navigable set and
 * map types in non-GWT only, while GWT code will inherit the SortedMap/SortedSet overrides.
 *
 * @author Louis Wasserman",8,* Basic implementation of a {@link SortedSetMultimap} with a sorted key set.,,"* <p>This superclass allows {@code TreeMultimap} to override methods to return navigable set and
  * map types in non-GWT only, while GWT code will inherit the SortedMap/SortedSet overrides.",,,,,,,,,,* @author Louis Wasserman,,,,,,,,,,
E1,E2 (agree),,,ArrayListMultimapGwtSerializationDependencies.java," * A dummy superclass to support GWT serialization of the element types of an {@link
 * ArrayListMultimap}. The GWT supersource for this class contains a field for each type.
 *
 * <p>For details about this hack, see {@link GwtSerializationDependencies}, which takes the same
 * approach but with a subclass rather than a superclass.
 *
 * <p>TODO(cpovirk): Consider applying this subclass approach to our other types.
 ",9,"* A dummy superclass to support GWT serialization of the element types of an {@link
  * ArrayListMultimap}.",The GWT supersource for this class contains a field for each type.,,,,,* <p>TODO(cpovirk): Consider applying this subclass approach to our other types.,,,,,,,"* <p>For details about this hack, see {@link GwtSerializationDependencies}, which takes the same
  * approach but with a subclass rather than a superclass.",,,,,,,,,
E1,E2 (agree),,,CloseablesTest.java,"
 * Unit tests for {@link Closeables}.
 *
 * <p>Checks proper closing behavior, and ensures that IOExceptions on Closeable.close() are not
 * propagated out from the {@link Closeables#close} method if {@code swallowException} is true.
 *
 * @author Michael Lancaster",8,* Unit tests for {@link Closeables}.,"* <p>Checks proper closing behavior, and ensures that IOExceptions on Closeable.close() are not
  * propagated out from the {@link Closeables#close} method if {@code swallowException} is true.",,,,,,,,,,,* @author Michael Lancaster,,,,,,,,,,
E1,E2 (agree),,,Resources.java,"* Provides utility methods for working with resources in the classpath. Note that even though these
 * methods use {@link URL} parameters, they are usually not appropriate for HTTP or other
 * non-classpath resources.
 *
 * <p>All method parameters must be non-null unless documented otherwise.
 *
 * @author Chris Nokleberg
 * @author Ben Yu
 * @author Colin Decker
 * @since 1.0
 | A byte source that reads from a URL using {@link URL#openStream()}.",12,* Provides utility methods for working with resources in the classpath.,A byte source that reads from a URL using {@link URL#openStream()},,* @since 1.0,* <p>All method parameters must be non-null unless documented otherwise.,,,,,,,,"* @author Chris Nokleberg
  * @author Ben Yu
  * @author Colin Decker",,,,"Note that even though these
  * methods use {@link URL} parameters, they are usually not appropriate for HTTP or other
  * non-classpath resources.",,,,,,
E1,E2 (agree),,,FeatureSpecificTestSuiteBuilder.java,"
 * Creates, based on your criteria, a JUnit test suite that exhaustively tests the object generated
 * by a G, selecting appropriate tests by matching them against specified features.
 *
 * @param <B> The concrete type of this builder (the 'self-type'). All the Builder methods of this
 *     class (such as {@link #named}) return this type, so that Builder methods of more derived
 *     classes can be chained onto them without casting.
 * @param <G> The type of the generator to be passed to testers in the generated test suite. An
 *     instance of G should somehow provide an instance of the class under test, plus any other
 *     information required to parameterize the test.
 * @author George van den Driessche",12,"* Creates, based on your criteria, a JUnit test suite that exhaustively tests the object generated
  * by a G",", selecting appropriate tests by matching them against specified features.",,,"* @param <B> The concrete type of this builder (the 'self-type'). All the Builder methods of this
  * class (such as {@link #named}) return this type, so that Builder methods of more derived
  * classes can be chained onto them without casting.
  * @param <G> The type of the generator to be passed to testers in the generated test suite. An
  * instance of G should somehow provide an instance of the class under test, plus any other
  * information required to parameterize the test.",,,,,,,,* @author George van den Driessche,,,,,,,,,,
E1,,E3 (disagree),,ArbitraryInstances.java,"* Supplies an arbitrary ""default"" instance for a wide range of types, often useful in testing
 * utilities.
 *
 * <p>Covers arrays, enums and common types defined in {@code java.lang}, {@code java.lang.reflect},
 * {@code java.io}, {@code java.nio}, {@code java.math}, {@code java.util}, {@code
 * java.util.concurrent}, {@code java.util.regex}, {@code com.google.common.base}, {@code
 * com.google.common.collect} and {@code com.google.common.primitives}. In addition, if the type
 * exposes at least one public static final constant of the same type, one of the constants will be
 * used; or if the class exposes a public parameter-less constructor then it will be ""new""d and
 * returned.
 *
 * <p>All default instances returned by {@link #get} are generics-safe. Clients won't get type
 * errors for using {@code get(Comparator.class)} as a {@code Comparator<Foo>}, for example.
 * Immutable empty instances are returned for collection types; {@code """"} for string; {@code 0} for
 * number types; reasonable default instance for other stateless types. For mutable types, a fresh
 * instance is created each time {@code get()} is called.
 *
 * @author Kevin Bourrillion
 * @author Ben Yu
 * @since 12.0
 | Compare by toString() to satisfy 2 properties:| 1. compareTo(null) should throw NullPointerException| Internal implementations of some classes, with public default constructor that get() needs.| Compare by toString() to satisfy 2 properties:| 1. compareTo(null) should throw NullPointerException| 2. the order is deterministic and easy to understand, for debugging purpose.| Compare by toString() to satisfy 2 properties:| 1. compareTo(null) should throw NullPointerException| Always equal is a valid total ordering. And it works for any Object.",22,"* Supplies an arbitrary ""default"" instance for a wide range of types, often useful in testing
  * utilities.","* <p>Covers arrays, enums and common types defined in {@code java.lang}, {@code java.lang.reflect},
  * {@code java.io}, {@code java.nio}, {@code java.math}, {@code java.util}, {@code
  * java.util.concurrent}, {@code java.util.regex}, {@code com.google.common.base}, {@code
  * com.google.common.collect} and {@code com.google.common.primitives}. In addition, if the type
  * exposes at least one public static final constant of the same type, one of the constants will be
  * used; or if the class exposes a public parameter-less constructor then it will be ""new""d and
  * returned.
  *",,* @since 12.0,"* <p>All default instances returned by {@link #get} are generics-safe. Clients won't get type
  * errors for using {@code get(Comparator.class)} as a {@code Comparator<Foo>}, for example.
  * Immutable empty instances are returned for collection types; {@code """"} for string; {@code 0} for
  * number types; reasonable default instance for other stateless types. For mutable types, a fresh
  * instance is created each time {@code get()} is called.","| Compare by toString() to satisfy 2 properties:| 1. compareTo(null) should throw NullPointerException| Internal implementations of some classes, with public default constructor that get() needs.| Compare by toString() to satisfy 2 properties:| 1. compareTo(null) should throw NullPointerException| 2. the order is deterministic and easy to understand, for debugging purpose.| Compare by toString() to satisfy 2 properties:| 1. compareTo(null) should throw NullPointerException| Always equal is a valid total ordering. And it works for any Object.",,,,,,,"* @author Kevin Bourrillion
  * @author Ben Yu",All default instances returned by {@link #get} are generics-safe.,,,"Clients won't get type
 * errors for using {@code get(Comparator.class)} as a {@code Comparator<Foo>}",,,,,,Always equal is a valid total ordering. And it works for any Object.
E1,,E3 (disagree),,Preconditions.java,"
 * Static convenience methods that help a method or constructor check whether it was invoked
 * correctly (that is, whether its <i>preconditions</i> were met).
 *
 * <p>If the precondition is not met, the {@code Preconditions} method throws an unchecked exception
 * of a specified type, which helps the method in which the exception was thrown communicate that
 * its caller has made a mistake. This allows constructs such as
 *
 * <pre>{@code
 * public static double sqrt(double value) {
 *   if (value < 0) {
 *     throw new IllegalArgumentException(""input is negative: "" + value);
 *   }
 *   // calculate square root
 * }
 * }</pre>
 *
 * <p>to be replaced with the more compact
 *
 * <pre>{@code
 * public static double sqrt(double value) {
 *   checkArgument(value >= 0, ""input is negative: %s"", value);
 *   // calculate square root
 * }
 * }</pre>
 *
 * <p>so that a hypothetical bad caller of this method, such as:
 *
 * <pre>{@code
 * void exampleBadCaller() {
 *   double d = sqrt(-1.0);
 * }
 * }</pre>
 *
 * <p>would be flagged as having called {@code sqrt()} with an illegal argument.
 *
 * <h3>Performance</h3>
 *
 * <p>Avoid passing message arguments that are expensive to compute; your code will always compute
 * them, even though they usually won't be needed. If you have such arguments, use the conventional
 * if/throw idiom instead.
 *
 * <p>Depending on your message arguments, memory may be allocated for boxing and varargs array
 * creation. However, the methods of this class have a large number of overloads that prevent such
 * allocations in many common cases.
 *
 * <p>The message string is not formatted unless the exception will be thrown, so the cost of the
 * string formatting itself should not be a concern.
 *
 * <p>As with any performance concerns, you should consider profiling your code (in a production
 * environment if possible) before spending a lot of effort on tweaking a particular element.
 *
 * <h3>Other types of preconditions</h3>
 *
 * <p>Not every type of precondition failure is supported by these methods. Continue to throw
 * standard JDK exceptions such as {@link java.util.NoSuchElementException} or {@link
 * UnsupportedOperationException} in the situations they are intended for.
 *
 * <h3>Non-preconditions</h3>
 *
 * <p>It is of course possible to use the methods of this class to check for invalid conditions
 * which are <i>not the caller's fault</i>. Doing so is <b>not recommended</b> because it is
 * misleading to future readers of the code and of stack traces. See <a
 * href=""https://github.com/google/guava/wiki/ConditionalFailuresExplained"">Conditional failures
 * explained</a> in the Guava User Guide for more advice. Notably, {@link Verify} offers assertions
 * similar to those in this class for non-precondition checks.
 *
 * <h3>{@code java.util.Objects.requireNonNull()}</h3>
 *
 * <p>Projects which use {@code com.google.common} should generally avoid the use of {@link
 * java.util.Objects#requireNonNull(Object)}. Instead, use whichever of {@link
 * #checkNotNull(Object)} or {@link Verify#verifyNotNull(Object)} is appropriate to the situation.
 * (The same goes for the message-accepting overloads.)
 *
 * <h3>Only {@code %s} is supported</h3>
 *
 * <p>{@code Preconditions} uses {@link Strings#lenientFormat} to format error message template
 * strings. This only supports the {@code ""%s""} specifier, not the full range of {@link
 * java.util.Formatter} specifiers. However, note that if the number of arguments does not match the
 * number of occurrences of {@code ""%s""} in the format string, {@code Preconditions} will still
 * behave as expected, and will still include all argument values in the error message; the message
 * will simply not be formatted exactly as intended.
 *
 * <h3>More information</h3>
 *
 * <p>See the Guava User Guide on <a
 * href=""https://github.com/google/guava/wiki/PreconditionsExplained"">using {@code
 * Preconditions}</a>.
 *
 * @author Kevin Bourrillion
 * @since 2.0
 ",92,"* Static convenience methods that help a method or constructor check whether it was invoked
  * correctly (that is, whether its <i>preconditions</i> were met)."," * <p>If the precondition is not met, the {@code Preconditions} method throws an unchecked exception
 * of a specified type, which helps the method in which the exception was thrown communicate that
 * its caller has made a mistake. This allows constructs such as
 *
 * <pre>{@code
 * public static double sqrt(double value) {
 *   if (value < 0) {
 *     throw new IllegalArgumentException(""input is negative: "" + value);
 *   }
 *   // calculate square root
 * }
 * }</pre>",,* @since 2.0,"This allows constructs such as
  *
  * <pre>{@code
  * public static double sqrt(double value) {
  * if (value < 0) {
  * throw new IllegalArgumentException(""input is negative: "" + value);
  * }
  * // calculate square root
  * }
  * }</pre>
  *
  * <p>to be replaced with the more compact
  *
  * <pre>{@code
  * public static double sqrt(double value) {
  * checkArgument(value >= 0, ""input is negative: %s"", value);
  * // calculate square root
  * }
  * }</pre>
  *
  * <p>so that a hypothetical bad caller of this method, such as:
  *
  * <pre>{@code
  * void exampleBadCaller() {
  * double d = sqrt(-1.0);
  * }
  * }</pre>
  *
  * <p>would be flagged as having called {@code sqrt()} with an illegal argument.
  *
  * <h3>Performance</h3>
  *
  * <p>Avoid passing message arguments that are expensive to compute; your code will always compute
  * them, even though they usually won't be needed. If you have such arguments, use the conventional
  * if/throw idiom instead.
  *
  * <p>Depending on your message arguments, memory may be allocated for boxing and varargs array
  * creation. However, the methods of this class have a large number of overloads that prevent such
  * allocations in many common cases.
  *
  * <p>The message string is not formatted unless the exception will be thrown, so the cost of the
  * string formatting itself should not be a concern.
  *
  * <p>As with any performance concerns, you should consider profiling your code (in a production
  * environment if possible) before spending a lot of effort on tweaking a particular element.
  *
  * <h3>Other types of preconditions</h3>
  *
  * <p>Not every type of precondition failure is supported by these methods. Continue to throw
  * standard JDK exceptions such as {@link java.util.NoSuchElementException} or {@link
  * UnsupportedOperationException} in the situations they are intended for.
  *
  * <h3>Non-preconditions</h3>
  *
  * <p>It is of course possible to use the methods of this class to check for invalid conditions
  * which are <i>not the caller's fault</i>. Doing so is <b>not recommended</b> because it is
  * misleading to future readers of the code and of stack traces. See <a
  * href=""https://github.com/google/guava/wiki/ConditionalFailuresExplained"">Conditional failures
  * explained</a> in the Guava User Guide for more advice. Notably, {@link Verify} offers assertions
  * similar to those in this class for non-precondition checks.
  *
  * <h3>{@code java.util.Objects.requireNonNull()}</h3>
  *
  * <p>Projects which use {@code com.google.common} should generally avoid the use of {@link
  * java.util.Objects#requireNonNull(Object)}. Instead, use whichever of {@link
  * #checkNotNull(Object)} or {@link Verify#verifyNotNull(Object)} is appropriate to the situation.
  * (The same goes for the message-accepting overloads.)
  *
  * <h3>Only {@code %s} is supported</h3>
  *
  * <p>{@code Preconditions} uses {@link Strings#lenientFormat} to format error message template
  * strings. This only supports the {@code ""%s""} specifier, not the full range of {@link
  * java.util.Formatter} specifiers. However, note that if the number of arguments does not match the
  * number of occurrences of {@code ""%s""} in the format string, {@code Preconditions} will still
  * behave as expected, and will still include all argument values in the error message; the message
  * will simply not be formatted exactly as intended.","* <p>If the precondition is not met, the {@code Preconditions} method throws an unchecked exception
  * of a specified type, which helps the method in which the exception was thrown communicate that
  * its caller has made a mistake.",,,,,,,* @author Kevin Bourrillion,"* <h3>More information</h3>
  *
  * <p>See the Guava User Guide on <a
  * href=""https://github.com/google/guava/wiki/PreconditionsExplained"">using {@code
  * Preconditions}</a>.
. See <a
 * href=""https://github.com/google/guava/wiki/ConditionalFailuresExplained"">Conditional failures
 * explained</a> in the Guava User Guide for more advice

. Notably, {@link Verify} offers assertions
 * similar to those in this class for non-precondition checks.",,,"Avoid passing message arguments that are expensive to compute; your code will always compute
 * them, even though they usually won't be needed. If you have such arguments, use the conventional
 * if/throw idiom instead.
Not every type of precondition failure is supported by these methods. Continue to throw
 * standard JDK exceptions such as {@link java.util.NoSuchElementException} or {@link
 * UnsupportedOperationException} in the situations they are intended for.
Projects which use {@code com.google.common} should generally avoid the use of {@link
 * java.util.Objects#requireNonNull(Object)}.","As with any performance concerns, you should consider profiling your code (in a production
 * environment if possible) before spending a lot of effort on tweaking a particular element.
 Doing so is <b>not recommended</b> because it is
 * misleading to future readers of the code and of stack traces
 * <p>Projects which use {@code com.google.common} should generally avoid the use of {@link
 * java.util.Objects#requireNonNull(Object)}. Instead, use whichever of {@link
 * #checkNotNull(Object)} or {@link Verify#verifyNotNull(Object)} is appropriate to the situation.
 * (The same goes for the message-accepting overloads.)","If the precondition is not met, the {@code Preconditions} method throws an unchecked exception
 * of a specified type, which helps the method in which the exception was thrown communicate that
 * its caller has made a mistake.",,,,
E1,,E3 (disagree),,HostAndPort.java,"
 * An immutable representation of a host and port.
 *
 * <p>Example usage:
 *
 * <pre>
 * HostAndPort hp = HostAndPort.fromString(""[2001:db8::1]"")
 *     .withDefaultPort(80)
 *     .requireBracketsForIPv6();
 * hp.getHost();   // returns ""2001:db8::1""
 * hp.getPort();   // returns 80
 * hp.toString();  // returns ""[2001:db8::1]:80""
 * </pre>
 *
 * <p>Here are some examples of recognized formats:
 *
 * <ul>
 *   <li>example.com
 *   <li>example.com:80
 *   <li>192.0.2.1
 *   <li>192.0.2.1:80
 *   <li>[2001:db8::1] - {@link #getHost()} omits brackets
 *   <li>[2001:db8::1]:80 - {@link #getHost()} omits brackets
 *   <li>2001:db8::1 - Use {@link #requireBracketsForIPv6()} to prohibit this
 * </ul>
 *
 * <p>Note that this is not an exhaustive list, because these methods are only concerned with
 * brackets, colons, and port numbers. Full validation of the host field (if desired) is the
 * caller's responsibility.
 *
 * @author Paul Marks
 * @since 10.0
 ",33,* An immutable representation of a host and port.,,,* @since 10.0,"* <p>Example usage:
  *
  * <pre>
  * HostAndPort hp = HostAndPort.fromString(""[2001:db8::1]"")
  * .withDefaultPort(80)
  * .requireBracketsForIPv6();
  * hp.getHost(); // returns ""2001:db8::1""
  * hp.getPort(); // returns 80
  * hp.toString(); // returns ""[2001:db8::1]:80""
  * </pre>
  *
  * <p>Here are some examples of recognized formats:
  *
  * <ul>
  * <li>example.com
  * <li>example.com:80
  * <li>192.0.2.1
  * <li>192.0.2.1:80
  * <li>[2001:db8::1] - {@link #getHost()} omits brackets
  * <li>[2001:db8::1]:80 - {@link #getHost()} omits brackets
  * <li>2001:db8::1 - Use {@link #requireBracketsForIPv6()} to prohibit this
  * </ul>
  *
  * <p>Note that this is not an exhaustive list, because these methods are only concerned with
  * brackets, colons, and port numbers. Full validation of the host field (if desired) is the
  * caller's responsibility.",,,,,,,,* @author Paul Marks,,,,"Note that this is not an exhaustive list, because these methods are only concerned with
 * brackets, colons, and port numbers. Full validation of the host field (if desired) is the
 * caller's responsibility.",,,,,,
E1,,E3 (disagree),,ClassToInstanceMap.java,"
 * A map, each entry of which maps a Java <a href=""http://tinyurl.com/2cmwkz"">raw type</a> to an
 * instance of that type. In addition to implementing {@code Map}, the additional type-safe
 * operations {@link #putInstance} and {@link #getInstance} are available.
 *
 * <p>Like any other {@code Map<Class, Object>}, this map may contain entries for primitive types,
 * and a primitive type and its corresponding wrapper type may map to different values.
 *
 * <p>See the Guava User Guide article on <a href=
 * ""https://github.com/google/guava/wiki/NewCollectionTypesExplained#classtoinstancemap""> {@code
 * ClassToInstanceMap}</a>.
 *
 * <p>To map a generic type to an instance of that type, use {@link
 * com.google.common.reflect.TypeToInstanceMap} instead.
 *
 * @param <B> the common supertype that all entries must share; often this is simply {@link Object}
 * @author Kevin Bourrillion
 * @since 2.0",19,"* A map, each entry of which maps a Java <a href=""http://tinyurl.com/2cmwkz"">raw type</a> to an
  * instance of that type. In addition to implementing {@code Map}, the additional type-safe
  * operations {@link #putInstance} and {@link #getInstance} are available.","* <p>Like any other {@code Map<Class, Object>}, this map may contain entries for primitive types,
  * and a primitive type and its corresponding wrapper type may map to different values.",,* @since 2.0,"* <p>To map a generic type to an instance of that type, use {@link
  * com.google.common.reflect.TypeToInstanceMap} instead.
  *
  * @param <B> the common supertype that all entries must share; often this is simply {@link Object}",,,,,,,,* @author Kevin Bourrillion,"* <p>See the Guava User Guide article on <a href=
  * ""https://github.com/google/guava/wiki/NewCollectionTypesExplained#classtoinstancemap""> {@code
  * ClassToInstanceMap}</a>.",,,,"To map a generic type to an instance of that type, use {@link
 * com.google.common.reflect.TypeToInstanceMap} instead.",,,,,
E1,,E3 (disagree),,MultimapBuilder.java,"
 * A builder for a multimap implementation that allows customization of the backing map and value
 * collection implementations used in a particular multimap.
 *
 * <p>This can be used to easily configure multimap data structure implementations not provided
 * explicitly in {@code com.google.common.collect}, for example:
 *
 * <pre>{@code
 * ListMultimap<String, Integer> treeListMultimap =
 *     MultimapBuilder.treeKeys().arrayListValues().build();
 * SetMultimap<Integer, MyEnum> hashEnumMultimap =
 *     MultimapBuilder.hashKeys().enumSetValues(MyEnum.class).build();
 * }</pre>
 *
 * <p>{@code MultimapBuilder} instances are immutable. Invoking a configuration method has no effect
 * on the receiving instance; you must store and use the new builder instance it returns instead.
 *
 * <p>The generated multimaps are serializable if the key and value types are serializable, unless
 * stated otherwise in one of the configuration methods.
 *
 * @author Louis Wasserman
 * @param <K0> An upper bound on the key type of the generated multimap.
 * @param <V0> An upper bound on the value type of the generated multimap.
 * @since 16.0
 |
   * Leaving K and V as upper bounds rather than the actual key and value types allows type
   * parameters to be left implicit more often. CacheBuilder uses the same technique.
   |
   * An intermediate stage in a {@link MultimapBuilder} in which the key-value collection map
   * implementation has been specified, but the value collection implementation has not.
   *
   * @param <K0> The upper bound on the key type of the generated multimap.
   * @since 16.0
   |
   * Leaving K and V as upper bounds rather than the actual key and value types allows type
   * parameters to be left implicit more often. CacheBuilder uses the same technique.
   |
   * A specialization of {@link MultimapBuilder} that generates {@link ListMultimap} instances.
   *
   * @since 16.0
   |
   * Leaving K and V as upper bounds rather than the actual key and value types allows type
   * parameters to be left implicit more often. CacheBuilder uses the same technique.
   |
   * A specialization of {@link MultimapBuilder} that generates {@link SetMultimap} instances.
   *
   * @since 16.0
   |
   * Leaving K and V as upper bounds rather than the actual key and value types allows type
   * parameters to be left implicit more often. CacheBuilder uses the same technique.
   |
   * A specialization of {@link MultimapBuilder} that generates {@link SortedSetMultimap} instances.
   *
   * @since 16.0
   ",55,"* A builder for a multimap implementation that allows customization of the backing map and value
  * collection implementations used in a particular multimap.","This can be used to easily configure multimap data structure implementations not provided
 * explicitly in {@code com.google.common.collect},",,* @since 16.0,"* <p>This can be used to easily configure multimap data structure implementations not provided
  * explicitly in {@code com.google.common.collect}, for example:
  *
  * <pre>{@code
  * ListMultimap<String, Integer> treeListMultimap =
  * MultimapBuilder.treeKeys().arrayListValues().build();
  * SetMultimap<Integer, MyEnum> hashEnumMultimap =
  * MultimapBuilder.hashKeys().enumSetValues(MyEnum.class).build();
  * }</pre>
  *
  * <p>{@code MultimapBuilder} instances are immutable. Invoking a configuration method has no effect
  * on the receiving instance; you must store and use the new builder instance it returns instead.
  *
  * <p>The generated multimaps are serializable if the key and value types are serializable, unless
  * stated otherwise in one of the configuration methods.
 
 
  * @param <K0> An upper bound on the key type of the generated multimap.
  * @param <V0> An upper bound on the value type of the generated multimap.
 
 |
  * Leaving K and V as upper bounds rather than the actual key and value types allows type
  * parameters to be left implicit more often. CacheBuilder uses the same technique.
  |
  * An intermediate stage in a {@link MultimapBuilder} in which the key-value collection map
  * implementation has been specified, but the value collection implementation has not.
  *
  * @param <K0> The upper bound on the key type of the generated multimap.
  * @since 16.0
  |
  * Leaving K and V as upper bounds rather than the actual key and value types allows type
  * parameters to be left implicit more often. CacheBuilder uses the same technique.
  |
  * A specialization of {@link MultimapBuilder} that generates {@link ListMultimap} instances.
  *",,,,,,,,* @author Louis Wasserman,,,"* @since 16.0
  |
  * Leaving K and V as upper bounds rather than the actual key and value types allows type
  * parameters to be left implicit more often. CacheBuilder uses the same technique.
  |
  * A specialization of {@link MultimapBuilder} that generates {@link SetMultimap} instances.
  *
  * @since 16.0
  |
  * Leaving K and V as upper bounds rather than the actual key and value types allows type
  * parameters to be left implicit more often. CacheBuilder uses the same technique.
  |
  * A specialization of {@link MultimapBuilder} that generates {@link SortedSetMultimap} instances.
  *
  * @since 16.0","Invoking a configuration method has no effect
 * on the receiving instance; you must store and use the new builder instance it returns instead.",,,,,,
E1,,E3 (disagree),,CacheBuilder.java,"
 * A builder of {@link LoadingCache} and {@link Cache} instances having any combination of the
 * following features:
 *
 * <ul>
 *   <li>automatic loading of entries into the cache
 *   <li>least-recently-used eviction when a maximum size is exceeded
 *   <li>time-based expiration of entries, measured since last access or last write
 *   <li>keys automatically wrapped in {@linkplain WeakReference weak} references
 *   <li>values automatically wrapped in {@linkplain WeakReference weak} or {@linkplain
 *       SoftReference soft} references
 *   <li>notification of evicted (or otherwise removed) entries
 *   <li>accumulation of cache access statistics
 * </ul>
 *
 *
 * <p>These features are all optional; caches can be created using all or none of them. By default
 * cache instances created by {@code CacheBuilder} will not perform any type of eviction.
 *
 * <p>Usage example:
 *
 * <pre>{@code
 * LoadingCache<Key, Graph> graphs = CacheBuilder.newBuilder()
 *     .maximumSize(10000)
 *     .expireAfterWrite(Duration.ofMinutes(10))
 *     .removalListener(MY_LISTENER)
 *     .build(
 *         new CacheLoader<Key, Graph>() {
 *           public Graph load(Key key) throws AnyException {
 *             return createExpensiveGraph(key);
 *           }
 *         });
 * }</pre>
 *
 * <p>Or equivalently,
 *
 * <pre>{@code
 * // In real life this would come from a command-line flag or config file
 * String spec = ""maximumSize=10000,expireAfterWrite=10m"";
 *
 * LoadingCache<Key, Graph> graphs = CacheBuilder.from(spec)
 *     .removalListener(MY_LISTENER)
 *     .build(
 *         new CacheLoader<Key, Graph>() {
 *           public Graph load(Key key) throws AnyException {
 *             return createExpensiveGraph(key);
 *           }
 *         });
 * }</pre>
 *
 * <p>The returned cache is implemented as a hash table with similar performance characteristics to
 * {@link ConcurrentHashMap}. It implements all optional operations of the {@link LoadingCache} and
 * {@link Cache} interfaces. The {@code asMap} view (and its collection views) have <i>weakly
 * consistent iterators</i>. This means that they are safe for concurrent use, but if other threads
 * modify the cache after the iterator is created, it is undefined which of these changes, if any,
 * are reflected in that iterator. These iterators never throw {@link
 * ConcurrentModificationException}.
 *
 * <p><b>Note:</b> by default, the returned cache uses equality comparisons (the {@link
 * Object#equals equals} method) to determine equality for keys or values. However, if {@link
 * #weakKeys} was specified, the cache uses identity ({@code ==}) comparisons instead for keys.
 * Likewise, if {@link #weakValues} or {@link #softValues} was specified, the cache uses identity
 * comparisons for values.
 *
 * <p>Entries are automatically evicted from the cache when any of {@linkplain #maximumSize(long)
 * maximumSize}, {@linkplain #maximumWeight(long) maximumWeight}, {@linkplain #expireAfterWrite
 * expireAfterWrite}, {@linkplain #expireAfterAccess expireAfterAccess}, {@linkplain #weakKeys
 * weakKeys}, {@linkplain #weakValues weakValues}, or {@linkplain #softValues softValues} are
 * requested.
 *
 * <p>If {@linkplain #maximumSize(long) maximumSize} or {@linkplain #maximumWeight(long)
 * maximumWeight} is requested entries may be evicted on each cache modification.
 *
 * <p>If {@linkplain #expireAfterWrite expireAfterWrite} or {@linkplain #expireAfterAccess
 * expireAfterAccess} is requested entries may be evicted on each cache modification, on occasional
 * cache accesses, or on calls to {@link Cache#cleanUp}. Expired entries may be counted by {@link
 * Cache#size}, but will never be visible to read or write operations.
 *
 * <p>If {@linkplain #weakKeys weakKeys}, {@linkplain #weakValues weakValues}, or {@linkplain
 * #softValues softValues} are requested, it is possible for a key or value present in the cache to
 * be reclaimed by the garbage collector. Entries with reclaimed keys or values may be removed from
 * the cache on each cache modification, on occasional cache accesses, or on calls to {@link
 * Cache#cleanUp}; such entries may be counted in {@link Cache#size}, but will never be visible to
 * read or write operations.
 *
 * <p>Certain cache configurations will result in the accrual of periodic maintenance tasks which
 * will be performed during write operations, or during occasional read operations in the absence of
 * writes. The {@link Cache#cleanUp} method of the returned cache will also perform maintenance, but
 * calling it should not be necessary with a high throughput cache. Only caches built with
 * {@linkplain #removalListener removalListener}, {@linkplain #expireAfterWrite expireAfterWrite},
 * {@linkplain #expireAfterAccess expireAfterAccess}, {@linkplain #weakKeys weakKeys}, {@linkplain
 * #weakValues weakValues}, or {@linkplain #softValues softValues} perform periodic maintenance.
 *
 * <p>The caches produced by {@code CacheBuilder} are serializable, and the deserialized caches
 * retain all the configuration properties of the original cache. Note that the serialized form does
 * <i>not</i> include cache contents, but only configuration.
 *
 * <p>See the Guava User Guide article on <a
 * href=""https://github.com/google/guava/wiki/CachesExplained"">caching</a> for a higher-level
 * explanation.
 *
 * @param <K> the most general key type this builder will be able to create caches for. This is
 *     normally {@code Object} unless it is constrained by using a method like {@code
 *     #removalListener}
 * @param <V> the most general value type this builder will be able to create caches for. This is
 *     normally {@code Object} unless it is constrained by using a method like {@code
 *     #removalListener}
 * @author Charles Fry
 * @author Kevin Bourrillion
 * @since 10.0
 ",111,* A builder of {@link LoadingCache} and {@link Cache} instances,"having any combination of the
  * following features:
  *
  * <ul>
  * <li>automatic loading of entries into the cache
  * <li>least-recently-used eviction when a maximum size is exceeded
  * <li>time-based expiration of entries, measured since last access or last write
  * <li>keys automatically wrapped in {@linkplain WeakReference weak} references
  * <li>values automatically wrapped in {@linkplain WeakReference weak} or {@linkplain
  * SoftReference soft} references
  * <li>notification of evicted (or otherwise removed) entries
  * <li>accumulation of cache access statistics
  * </ul>
* <p>Entries are automatically evicted from the cache when any of {@linkplain #maximumSize(long)
 * maximumSize}, {@linkplain #maximumWeight(long) maximumWeight}, {@linkplain #expireAfterWrite
 * expireAfterWrite}, {@linkplain #expireAfterAccess expireAfterAccess}, {@linkplain #weakKeys
 * weakKeys}, {@linkplain #weakValues weakValues}, or {@linkplain #softValues softValues} are
 * requested.",,* @since 10.0,"* <p>These features are all optional; caches can be created using all or none of them. By default
  * cache instances created by {@code CacheBuilder} will not perform any type of eviction.
  *
  * <p>Usage example:
  *
  * <pre>{@code
  * LoadingCache<Key, Graph> graphs = CacheBuilder.newBuilder()
  * .maximumSize(10000)
  * .expireAfterWrite(Duration.ofMinutes(10))
  * .removalListener(MY_LISTENER)
  * .build(
  * new CacheLoader<Key, Graph>() {
  * public Graph load(Key key) throws AnyException {
  * return createExpensiveGraph(key);
  * }
  * });
  * }</pre>
  *
  * <p>Or equivalently,
  *
  * <pre>{@code
  * // In real life this would come from a command-line flag or config file
  * String spec = ""maximumSize=10000,expireAfterWrite=10m"";
  *
  * LoadingCache<Key, Graph> graphs = CacheBuilder.from(spec)
  * .removalListener(MY_LISTENER)
  * .build(
  * new CacheLoader<Key, Graph>() {
  * public Graph load(Key key) throws AnyException {
  * return createExpensiveGraph(key);
  * }
  * });
  * }</pre>
  *
  * <p>The returned cache is implemented as a hash table with similar performance characteristics to
  * {@link ConcurrentHashMap}. It implements all optional operations of the {@link LoadingCache} and
  * {@link Cache} interfaces. The {@code asMap} view (and its collection views) have <i>weakly
  * consistent iterators</i>. This means that they are safe for concurrent use, but if other threads
  * modify the cache after the iterator is created, it is undefined which of these changes, if any,
  * are reflected in that iterator. These iterators never throw {@link
  * ConcurrentModificationException}.
  *
  * <p><b>Note:</b> by default, the returned cache uses equality comparisons (the {@link
  * Object#equals equals} method) to determine equality for keys or values. However, if {@link
  * #weakKeys} was specified, the cache uses identity ({@code ==}) comparisons instead for keys.
  * Likewise, if {@link #weakValues} or {@link #softValues} was specified, the cache uses identity
  * comparisons for values.
  *
  * <p>Entries are automatically evicted from the cache when any of {@linkplain #maximumSize(long)
  * maximumSize}, {@linkplain #maximumWeight(long) maximumWeight}, {@linkplain #expireAfterWrite
  * expireAfterWrite}, {@linkplain #expireAfterAccess expireAfterAccess}, {@linkplain #weakKeys
  * weakKeys}, {@linkplain #weakValues weakValues}, or {@linkplain #softValues softValues} are
  * requested.
  *
  * <p>If {@linkplain #maximumSize(long) maximumSize} or {@linkplain #maximumWeight(long)
  * maximumWeight} is requested entries may be evicted on each cache modification.
  *
  * <p>If {@linkplain #expireAfterWrite expireAfterWrite} or {@linkplain #expireAfterAccess
  * expireAfterAccess} is requested entries may be evicted on each cache modification, on occasional
  * cache accesses, or on calls to {@link Cache#cleanUp}. Expired entries may be counted by {@link
  * Cache#size}, but will never be visible to read or write operations.
  *
  * <p>If {@linkplain #weakKeys weakKeys}, {@linkplain #weakValues weakValues}, or {@linkplain
  * #softValues softValues} are requested, it is possible for a key or value present in the cache to
  * be reclaimed by the garbage collector. Entries with reclaimed keys or values may be removed from
  * the cache on each cache modification, on occasional cache accesses, or on calls to {@link
  * Cache#cleanUp}; such entries may be counted in {@link Cache#size}, but will never be visible to
  * read or write operations.
  *
  * <p>Certain cache configurations will result in the accrual of periodic maintenance tasks which
  * will be performed during write operations, or during occasional read operations in the absence of
  * writes. The {@link Cache#cleanUp} method of the returned cache will also perform maintenance, but
  * calling it should not be necessary with a high throughput cache. Only caches built with
  * {@linkplain #removalListener removalListener}, {@linkplain #expireAfterWrite expireAfterWrite},
  * {@linkplain #expireAfterAccess expireAfterAccess}, {@linkplain #weakKeys weakKeys}, {@linkplain
  * #weakValues weakValues}, or {@linkplain #softValues softValues} perform periodic maintenance.
  *
  * <p>The caches produced by {@code CacheBuilder} are serializable, and the deserialized caches
  * retain all the configuration properties of the original cache. Note that the serialized form does
  * <i>not</i> include cache contents, but only configuration.
 
  * @param <K> the most general key type this builder will be able to create caches for. This is
  * normally {@code Object} unless it is constrained by using a method like {@code
  * #removalListener}
  * @param <V> the most general value type this builder will be able to create caches for. This is
  * normally {@code Object} unless it is constrained by using a method like {@code
  * #removalListener}",,,,,,,,"* @author Charles Fry
  * @author Kevin Bourrillion","* <p>See the Guava User Guide article on <a
  * href=""https://github.com/google/guava/wiki/CachesExplained"">caching</a> for a higher-level
  * explanation.
These iterators never throw {@link
 * ConcurrentModificationException}.",,," * <p><b>Note:</b> by default, the returned cache uses equality comparisons (the {@link
 * Object#equals equals} method) to determine equality for keys or values. However, if {@link
 * #weakKeys} was specified, the cache uses identity ({@code ==}) comparisons instead for keys.
 * Likewise, if {@link #weakValues} or {@link #softValues} was specified, the cache uses identity
 * comparisons for values.
 Note that the serialized form does
 * <i>not</i> include cache contents, but only configuration.",,,,,,"Certain cache configurations will result in the accrual of periodic maintenance tasks which
 * will be performed during write operations, or during occasional read operations in the absence of
 * writes."
E1,,E3 (disagree),,AbstractLoadingCache.java," * This class provides a skeletal implementation of the {@code Cache} interface to minimize the
 * effort required to implement this interface.
 *
 * <p>To implement a cache, the programmer needs only to extend this class and provide an
 * implementation for the {@link #get(Object)} and {@link #getIfPresent} methods. {@link
 * #getUnchecked}, {@link #get(Object, Callable)}, and {@link #getAll} are implemented in terms of
 * {@code get}; {@link #getAllPresent} is implemented in terms of {@code getIfPresent}; {@link
 * #putAll} is implemented in terms of {@link #put}, {@link #invalidateAll(Iterable)} is implemented
 * in terms of {@link #invalidate}. The method {@link #cleanUp} is a no-op. All other methods throw
 * an {@link UnsupportedOperationException}.
 *
 * @author Charles Fry
 * @since 11.0",15,* This class provides a skeletal implementation of the {@code Cache} interface,,"to minimize the
  * effort required to implement this interface.",* @since 11.0,"* <p>To implement a cache, the programmer needs only to extend this class and provide an
  * implementation for the {@link #get(Object)} and {@link #getIfPresent} methods. {@link
  * #getUnchecked}, {@link #get(Object, Callable)}, and {@link #getAll} are implemented in terms of
  * {@code get}; {@link #getAllPresent} is implemented in terms of {@code getIfPresent}; {@link
  * #putAll} is implemented in terms of {@link #put}, {@link #invalidateAll(Iterable)} is implemented
  * in terms of {@link #invalidate}. The method {@link #cleanUp} is a no-op. All other methods throw
  * an {@link UnsupportedOperationException}.","All other methods throw
 * an {@link UnsupportedOperationException}.",,,,,,,* @author Charles Fry,,,,,,,,,,
E1,,E3 (disagree),,EqualsTester.java,"
 * Tester for equals() and hashCode() methods of a class.
 *
 * <p>The simplest use case is:
 *
 * <pre>
 * new EqualsTester().addEqualityGroup(foo).testEquals();
 * </pre>
 *
 * <p>This tests {@code foo.equals(foo)}, {@code foo.equals(null)}, and a few other operations.
 *
 * <p>For more extensive testing, add multiple equality groups. Each group should contain objects
 * that are equal to each other but unequal to the objects in any other group. For example:
 *
 * <pre>
 * new EqualsTester()
 *     .addEqualityGroup(new User(""page""), new User(""page""))
 *     .addEqualityGroup(new User(""sergey""))
 *     .testEquals();
 * </pre>
 *
 * <p>This tests:
 *
 * <ul>
 *   <li>comparing each object against itself returns true
 *   <li>comparing each object against null returns false
 *   <li>comparing each object against an instance of an incompatible class returns false
 *   <li>comparing each pair of objects within the same equality group returns true
 *   <li>comparing each pair of objects from different equality groups returns false
 *   <li>the hash codes of any two equal objects are equal
 * </ul>
 *
 * <p>When a test fails, the error message labels the objects involved in the failed comparison as
 * follows:
 *
 * <ul>
 *   <li>""{@code [group }<i>i</i>{@code , item }<i>j</i>{@code ]}"" refers to the
 *       <i>j</i><sup>th</sup> item in the <i>i</i><sup>th</sup> equality group, where both equality
 *       groups and the items within equality groups are numbered starting from 1. When either a
 *       constructor argument or an equal object is provided, that becomes group 1.
 * </ul>
 *
 * @author Jim McMaster
 * @author Jige Yu
 * @since 10.0
 |
   * Class used to test whether equals() correctly handles an instance of an incompatible class.
   * Since it is a private inner class, the invoker can never pass in an instance to the tester
   ",49,* Tester for equals() and hashCode() methods of a class.,,"* <p>This tests:
  *
  * <ul>
  * <li>comparing each object against itself returns true
  * <li>comparing each object against null returns false
  * <li>comparing each object against an instance of an incompatible class returns false
  * <li>comparing each pair of objects within the same equality group returns true
  * <li>comparing each pair of objects from different equality groups returns false
  * <li>the hash codes of any two equal objects are equal
  * </ul>",* @since 10.0,"* <p>The simplest use case is:
  *
  * <pre>
  * new EqualsTester().addEqualityGroup(foo).testEquals();
  * </pre>
  *
  * <p>This tests {@code foo.equals(foo)}, {@code foo.equals(null)}, and a few other operations.
  *
  * <p>For more extensive testing, add multiple equality groups. Each group should contain objects
  * that are equal to each other but unequal to the objects in any other group. For example:
  *
  * <pre>
  * new EqualsTester()
  * .addEqualityGroup(new User(""page""), new User(""page""))
  * .addEqualityGroup(new User(""sergey""))
  * .testEquals();
  * </pre>
 
 
  |
  * Class used to test whether equals() correctly handles an instance of an incompatible class.
  * Since it is a private inner class, the invoker can never pass in an instance to the tester","* <p>When a test fails, the error message labels the objects involved in the failed comparison as
  * follows:
  *
  * <ul>
  * <li>""{@code [group }<i>i</i>{@code , item }<i>j</i>{@code ]}"" refers to the
  * <i>j</i><sup>th</sup> item in the <i>i</i><sup>th</sup> equality group, where both equality
  * groups and the items within equality groups are numbered starting from 1. When either a
  * constructor argument or an equal object is provided, that becomes group 1.
  * </ul>",,,,,,,"* @author Jim McMaster
  * @author Jige Yu",,,,"   * Since it is a private inner class, the invoker can never pass in an instance to the tester","For more extensive testing, add multiple equality groups. Each group should contain objects
 * that are equal to each other but unequal to the objects in any other group",,,,,   * Class used to test whether equals() correctly handles an instance of an incompatible class.